{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import datetime\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants that we will use\n",
    "PROJECT_PATH = '/Users/chris/programming/kaggle-predict-future-sales'\n",
    "\n",
    "TRAIN = 'train'\n",
    "VALID = 'valid'\n",
    "TEST = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw CSV files\n",
    "\n",
    "csv_data = dict()\n",
    "\n",
    "# Import all csv file data\n",
    "csv_data[ 'sales_daily' ] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/sales_train.csv') )\n",
    "csv_data['item_cat'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/item_categories.csv') )\n",
    "csv_data['item'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/items.csv') )\n",
    "csv_data['sub'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/sample_submission.csv') )\n",
    "csv_data['shops'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/shops.csv') )\n",
    "csv_data['test'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/test.csv') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some new columns and aggregate from daily to monthly\n",
    "############################################################\n",
    "\n",
    "# Add the daily revenue\n",
    "csv_data['sales_daily'][ \"revenue\" ] = csv_data['sales_daily'].item_price * csv_data['sales_daily'].item_cnt_day\n",
    "\n",
    "# Aggregate the monthly data\n",
    "agg_rules = {'item_price' : \"mean\", \"revenue\" : \"sum\", \"item_cnt_day\" : \"sum\" }\n",
    "groupby_cols = [ 'date_block_num', \"item_id\", \"shop_id\" ]\n",
    "\n",
    "# Add the effective item price\n",
    "csv_data['sales_monthly'] = csv_data['sales_daily'].groupby( groupby_cols ).agg( agg_rules ).reset_index()\n",
    "\n",
    "# Rename the column to reflect monthly data\n",
    "csv_data['sales_monthly'].rename( columns={ 'item_cnt_day' : 'item_cnt_month'}, inplace=True )\n",
    "\n",
    "# Add unit price as a column\n",
    "csv_data['sales_monthly']['item_price_unit'] = np.round( csv_data['sales_monthly']['revenue'] / \\\n",
    "                              ( 1e-6 + csv_data['sales_monthly']['item_cnt_month'] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all combinations of shops/items for each date\n",
    "############################################################\n",
    "\n",
    "df_monthly = csv_data['sales_monthly']\n",
    "dates = df_monthly['date_block_num'].unique()\n",
    "\n",
    "df_id = pd.DataFrame( [], columns=['date_block_num', 'shop_id', 'item_id'])\n",
    "for dt in dates:\n",
    "        \n",
    "    df_t = df_monthly[ df_monthly['date_block_num'] == dt ]\n",
    "    uniq_shops = df_t['shop_id'].unique()    \n",
    "    uniq_items = df_t['item_id'].unique()\n",
    "        \n",
    "    new_rows = pd.DataFrame( itertools.product( [dt], uniq_shops, uniq_items ), columns=df_id.columns )\n",
    "    df_id = pd.concat( [ df_id, new_rows ], sort=False, axis=0 )\n",
    "\n",
    "# Join the test IDs to the data frame\n",
    "df_t = csv_data['test'].copy()\n",
    "df_t['date_block_num'] = 1 + df_id['date_block_num'].max()\n",
    "df_id = pd.concat( [ df_id, df_t ], sort=False, axis=0 ).drop(['ID'], axis=1)\n",
    "\n",
    "# Create a data frame using all the shop/item pairs from each month\n",
    "df_sales = df_id.merge( csv_data['sales_monthly'], on=['date_block_num', 'shop_id', 'item_id' ], how='left' )\n",
    "\n",
    "# Set missing values for revenue and item count to 0\n",
    "df_sales.loc[ np.isnan(df_sales['revenue']), ['revenue'] ] = 0\n",
    "df_sales.loc[ np.isnan(df_sales['item_cnt_month']), ['item_cnt_month'] ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns to the data frame\n",
    "############################################################\n",
    "\n",
    "# Add the category id\n",
    "df_sales = df_sales.merge( csv_data['item'], on='item_id' )\n",
    "df_sales = df_sales.drop('item_name', axis=1 )\n",
    "\n",
    "# Add a column combining shop and item, which together with date_block_num is a unique id\n",
    "df_sales['shop_item_id'] = df_sales['shop_id'] + 100 * df_sales['item_id']\n",
    "\n",
    "# Add the month and year\n",
    "df_sales['month'] = 1 + (df_sales['date_block_num'] % 12)\n",
    "df_sales['year'] = 2013 + (df_sales['date_block_num'] // 12)\n",
    "\n",
    "# Add the dates\n",
    "dates = pd.Series( [ datetime.date( y, m, 15 ) for y in range(2013,2016) for m in range(1, 13) ], \\\n",
    "                          index=range(0,36) )\n",
    "df_sales['date'] = df_sales['date_block_num'].map(dates)\n",
    "\n",
    "# Rename the target column and make it the first column\n",
    "target = df_sales.loc[:,['item_cnt_month']]\n",
    "target.columns = [ 'TARGET']\n",
    "df_sales = pd.concat( [ target, df_sales.drop('item_cnt_month', axis=1) ], axis=1 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to help creating pivot tables\n",
    "\n",
    "def create_pivot_ts( input_table, pivot_column, val_column, agg_rule, fill_method, min_date_block_num=0 ):\n",
    "    \n",
    "    df = input_table[ input_table['date_block_num'] >= min_date_block_num ]\n",
    "    \n",
    "    if isinstance( pivot_column, str ):\n",
    "        pivot_columns = [ pivot_column ]\n",
    "    else:\n",
    "        pivot_columns = pivot_column\n",
    "    \n",
    "    # Make time series out of the monthly  sales\n",
    "    tmp_table = df.groupby( [ 'date_block_num' ] + pivot_columns ).agg( { val_column : agg_rule } ).reset_index()\n",
    "    ts = tmp_table.pivot_table( val_column, index=\"date_block_num\", columns=pivot_columns )\n",
    "\n",
    "    # Fill missing values with 0\n",
    "    if fill_method == \"zero\":\n",
    "        ts = ts.fillna(0)\n",
    "    elif fill_method == 'ffill':\n",
    "        ts = ts.fillna(method=missing_method)\n",
    "    elif fill_method != 'none':\n",
    "        ValueError( 'Unsupported value: .' + missing_method )\n",
    "\n",
    "    # Set negative values to 0\n",
    "    ts[ ts < 0 ] = 0\n",
    "\n",
    "    # Make sure the dates are sorted\n",
    "    ts.sort_index(axis=0, ascending=True, inplace=True )\n",
    "    \n",
    "    return(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table with shop/item combinations and monthly sales\n",
    "ts_sales_raw = create_pivot_ts( df_sales, pivot_column=[ 'shop_item_id' ], \\\n",
    "    val_column='TARGET', agg_rule='sum', fill_method='none', min_date_block_num=0 )\n",
    "ts_sales = ts_sales_raw.fillna(0)\n",
    "\n",
    "# The last date is just test data, so we don't know the feature values\n",
    "ts_sales_raw.iloc[-1,:] = np.nan\n",
    "ts_sales.iloc[-1,:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table with shop/item combinations and monthly sales\n",
    "ts_sales_raw = create_pivot_ts( df_sales, pivot_column=[ 'shop_id', 'item_id', 'item_category_id' ], \\\n",
    "    val_column='item_cnt_month', agg_rule='sum', fill_method='none', min_date_block_num=0 )\n",
    "ts_sales = ts_sales_raw.fillna(0)\n",
    "\n",
    "# The last date is just test data, so we don't know the feature values\n",
    "ts_sales_raw.iloc[-1,:] = np.nan\n",
    "ts_sales.iloc[-1,:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table with shop/item combinations and monthly revenues\n",
    "ts_revenue_raw = create_pivot_ts( df_sales, pivot_column=[ 'shop_id', 'item_category_id', 'item_id' ], \\\n",
    "    val_column='revenue', agg_rule='sum', fill_method='none', min_date_block_num=0 )\n",
    "ts_revenue = ts_revenue_raw.fillna(0)\n",
    "\n",
    "# The last date is just test data, so we don't know the feature values\n",
    "ts_revenue_raw.iloc[-1,:] = np.nan\n",
    "ts_revenue.iloc[-1,:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating lagged sales...\n",
      "Calculating rolling mean sales...\n",
      "Calculating std. dev. of sales...\n",
      "Calculating quantiles of sales...\n",
      "Calculating categorical features...\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store features. We will concatenate these afterwards with hstack\n",
    "feature_vals = []\n",
    "feature_names = []\n",
    "cat_features = []\n",
    "\n",
    "# Just keep the last M months of observations, and convert to a numpy column vector\n",
    "months_to_keep = 10\n",
    "process_features = lambda x : x.iloc[-months_to_keep:,:].to_numpy().ravel()[:,np.newaxis]\n",
    "\n",
    "# Get the target - monthly sales, the next month into the future`\n",
    "ts = ts_sales.shift(periods=-1)\n",
    "feature_vals.append( process_features(ts) )\n",
    "feature_names.append( 'TARGET' )\n",
    "\n",
    "# Get lagged monthly sales\n",
    "print( 'Calculating lagged sales...')\n",
    "lags = [1, 2, 3, 4, 5, 6, 9, 12]\n",
    "for L in lags:\n",
    "    ts = ts_sales.shift(periods=L-1)\n",
    "    feature_vals.append( process_features(ts) )\n",
    "    feature_names.append( 'sales_lag_{:02}'.format(L) )\n",
    "\n",
    "# Get lagged monthly mean sales\n",
    "print( 'Calculating rolling mean sales...')\n",
    "means = [2, 3, 4, 5, 6, 9, 12]\n",
    "for M in means:\n",
    "    ts = ts_sales.rolling(window=M).mean()\n",
    "    feature_vals.append( process_features(ts) )    \n",
    "    feature_names.append( 'sales_mean_{:02}'.format(M) )\n",
    "\n",
    "# Get 12-month standard deviation\n",
    "print( 'Calculating std. dev. of sales...')\n",
    "ts = ts_sales.rolling(window=12).std()\n",
    "feature_vals.append( process_features(ts) )\n",
    "feature_names.append( 'sales_std_12' )\n",
    "\n",
    "# Get the 12-month quartiles\n",
    "print( 'Calculating quantiles of sales...')\n",
    "for p in [ 0.25, 0.50, 0.75]:\n",
    "    ts = ts_sales.rolling(window=12).quantile(p)\n",
    "    feature_vals.append( process_features(ts) )    \n",
    "    feature_names.append( 'sales_percentile_{:02}'.format(int(p * 100) ) )\n",
    "\n",
    "# Add some categorical features\n",
    "print( 'Calculating categorical features...')\n",
    "for j, col in enumerate(ts_sales.columns.names):\n",
    "    ts = np.array([ x[j] for x in ts_sales.columns ] * ts_sales.shape[0]).reshape( ts_sales.shape[0], ts_sales.shape[1])\n",
    "    feature_vals.append( process_features(pd.DataFrame(ts) ) )  \n",
    "    feature_names.append( col )\n",
    "    cat_features.append(col)\n",
    "\n",
    "# Add some more categorical features for the date block, year and month\n",
    "ts_dates = np.tile( np.array(ts_sales.index)[:,np.newaxis], ts_sales.shape[1] )\n",
    "feature_vals.append( process_features(pd.DataFrame(ts_dates) ) )\n",
    "feature_vals.append( process_features(pd.DataFrame( 1 + ( ts_dates % 12 ) ) ) )\n",
    "feature_vals.append( process_features(pd.DataFrame( 2013 + ( ts_dates // 12 ) ) ) )\n",
    "feature_names = feature_names + [ 'date_block_num', 'month', 'year' ]\n",
    "cat_features = cat_features + [ 'date_block_num', 'month', 'year' ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_months_since_first_and_last_observation( input_ts ):\n",
    "    \"\"\"Gets the number of months since the first and last observation in each columns, \n",
    "    at each point in time. This function has no 'look-forward' bias.\"\"\"\n",
    "    \n",
    "    # Create an array with the index (0 to T) of any non-zeros entries\n",
    "    month_of_obs = input_ts.to_numpy() * np.arange(1,input_ts.shape[0]+1)[:,np.newaxis]\n",
    "    month_of_obs = month_of_obs.astype('float32')\n",
    "    month_of_obs[ month_of_obs < 0.01 ] = np.nan\n",
    "\n",
    "    # Make a data frame\n",
    "    df_month_of_obs = pd.DataFrame( month_of_obs, columns=input_ts.columns)\n",
    "\n",
    "    # Loop through the time steps and find the months since first/last action at each t\n",
    "    months_since_first_obs = np.nan * np.ones_like(month_of_obs)\n",
    "    months_since_last_obs = np.nan * np.ones_like(month_of_obs)\n",
    "    for t in range( month_of_obs.shape[0] ):\n",
    "        if t % 10 == 0:\n",
    "            print(t)\n",
    "        months_since_first_obs[t,:] = t - df_month_of_obs.iloc[:(t+1),:].idxmin(axis=0)\n",
    "        months_since_last_obs[t,:] = t - df_month_of_obs.iloc[:(t+1),:].idxmax(axis=0)    \n",
    "\n",
    "    # Fill NaN's with 999 for shop/items that have never seen a sale\n",
    "    months_since_first_obs[ np.isnan( months_since_first_obs ) ] = 999\n",
    "    months_since_last_obs[ np.isnan( months_since_last_obs ) ] = 999\n",
    "\n",
    "    # Convert back into pandas Dataframe\n",
    "    months_since_first_obs = pd.DataFrame( months_since_first_obs, columns=ts_sales.columns )\n",
    "    months_since_last_obs = pd.DataFrame( months_since_last_obs, columns=ts_sales.columns )\n",
    "    \n",
    "    return months_since_first_obs, months_since_last_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/anaconda3/envs/python37/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in less\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# Get the number of months since the first and last sale, at each point in time\n",
    "months_since_first_sale, months_since_last_sale = \\\n",
    "        get_months_since_first_and_last_observation( ts_sales )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to features\n",
    "feature_vals.append( process_features( months_since_first_sale ) )\n",
    "feature_names.append( 'months_since_first_sale' )\n",
    "feature_vals.append( process_features( months_since_last_sale ) )\n",
    "feature_names.append( 'months_since_last_sale' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_total_by_shop = ts_sales_raw.groupby( level='shop_id', axis=1).transform('sum')\n",
    "sales_total_by_item = ts_sales_raw.groupby( level='item_id', axis=1).transform('sum')\n",
    "sales_total_by_cat = ts_sales_raw.groupby( level='item_category_id', axis=1).transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_count_by_shop = ts_sales_raw.groupby( level='shop_id', axis=1).transform('count')\n",
    "sales_count_by_item = ts_sales_raw.groupby( level='item_id', axis=1).transform('count')\n",
    "sales_count_by_cat = ts_sales_raw.groupby( level='item_category_id', axis=1).transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1823324,)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.groupby( [ 'date_block_num', 'shop_id' ] )['item_id'].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214200"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the features into a pandas data frame\n",
    "features_list = [ x for x in feature_vals ]\n",
    "df_full = pd.DataFrame( np.hstack(features_list), columns=feature_names)\n",
    "\n",
    "# Remove all rows with NA's (except when they occur in the TARGET column)\n",
    "na_locs = df_full.isna()\n",
    "df_full = df_full[~np.any(na_locs.iloc[:,1:], axis=1 ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcast the categorical features to int32, and other columns to float32\n",
    "\n",
    "for col in list(df_full.columns):\n",
    "    if col in cat_features:\n",
    "        df_full[col] =  df_full[col].astype('int32')\n",
    "    else:\n",
    "        df_full[col] =  df_full[col].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw features to a HDF5 file\n",
    "file_name = os.path.join( PROJECT_PATH, 'preprocessed_data/raw_features.h5')\n",
    "df_full.to_hdf(file_name, key='raw_features')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw features from a HDF5 file\n",
    "#cat_features = [ 'shop_id', 'item_id', 'date_block_num', 'month', 'year']\n",
    "# file_name = os.path.join( PROJECT_PATH, 'preprocessed_data/raw_features.h5')\n",
    "# df_full = pd.read_hdf(file_name, key='raw_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the test set, just keep shop/item combinations required in the output\n",
    "df_test = df_full.merge( csv_data['test'], on=['shop_id', 'item_id'], how='left' )\n",
    "df_test = df_test[ ~np.isnan(df_test['ID'] ) ]\n",
    "df_test.sort_values( [ 'date_block_num', 'ID' ], inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Months since first sale: 0.479906629318394\n",
      "Avg. months since first sale: 15.133522987365723\n",
      "Median months since first sale: 13.0\n",
      "Months since last sale: 0.479906629318394\n",
      "Avg. months since last sale: 5.357895374298096\n",
      "Median months since last sale: 2.0\n"
     ]
    }
   ],
   "source": [
    "dt = data_sets[-1][-2]\n",
    "fs = np.count_nonzero( dt['months_since_first_sale'] == 999 ) / len(dt)\n",
    "print( 'Months since first sale: {}'.format(fs) )\n",
    "tmp = dt['months_since_first_sale'][ dt['months_since_first_sale'] != 999 ].mean()\n",
    "print( 'Avg. months since first sale: {}'.format(tmp) )\n",
    "tmp = dt['months_since_first_sale'][ dt['months_since_first_sale'] != 999 ].median()\n",
    "print( 'Median months since first sale: {}'.format(tmp) )\n",
    "\n",
    "fs = np.count_nonzero( dt['months_since_last_sale'] == 999 ) / len(dt)\n",
    "print( 'Months since last sale: {}'.format(fs) )\n",
    "tmp = dt['months_since_last_sale'][ dt['months_since_last_sale'] != 999 ].mean()\n",
    "print( 'Avg. months since last sale: {}'.format(tmp) )\n",
    "tmp = dt['months_since_last_sale'][ dt['months_since_last_sale'] != 999 ].median()\n",
    "print( 'Median months since last sale: {}'.format(tmp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32414973785176016"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan( ts_sales_raw ).sum(axis=0)\n",
    "#df_agg = csv_data['sales_monthly'].groupby( ['shop_id', 'item_id' ]).sum()\n",
    "len(df_agg) / ( len(csv_data['sales_monthly']['item_id'].unique()) * len(csv_data['sales_monthly']['shop_id'].unique()) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get different datasets, in chronological order\n",
    "# The test set is always the next period after the end of the train set.\n",
    "##########################################################################\n",
    "\n",
    "def get_validation_set( df, idx, min_train_size=4 ):\n",
    "\n",
    "    date_blocks = df['date_block_num']\n",
    "    uniq_date_blocks = np.sort( date_blocks.unique() )\n",
    "\n",
    "    n_datasets = len(uniq_date_blocks) - min_train_size - 1\n",
    "    if idx >= 0:\n",
    "        split_date = uniq_date_blocks[min_train_size + idx]\n",
    "    else:\n",
    "        split_date = uniq_date_blocks[idx]    \n",
    "\n",
    "    xtrain = df[ date_blocks < split_date].drop('TARGET', axis=1 )\n",
    "    ytrain = df[ date_blocks < split_date ]['TARGET'][:,np.newaxis]\n",
    "    xtest = df[ date_blocks == split_date ].drop('TARGET', axis=1 )\n",
    "    ytest = df[ date_blocks == split_date ]['TARGET'][:,np.newaxis]\n",
    "\n",
    "    return xtrain, ytrain, xtest, ytest\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "# Construct some train/validation/test data sets\n",
    "data_sets = []\n",
    "for j in range(100):\n",
    "    try:\n",
    "        xtrain, ytrain, xtest, ytest = get_validation_set( df_full, j, min_train_size=4 )\n",
    "        data_sets.append( ( xtrain, ytrain, xtest, ytest ) )\n",
    "    except IndexError:\n",
    "        break\n",
    "\n",
    "# Replace the last validation set with the test set\n",
    "_, _, xtest, ytest = get_validation_set( df_test.drop('ID', axis=1), j-1, min_train_size=4 )\n",
    "data_sets[-1] = ( data_sets[-1][0], data_sets[-1][1], xtest, ytest )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_forecast_to_file( yhat, X_test, output_file ):\n",
    "    \n",
    "    if isinstance( yhat, pd.Series ):\n",
    "        yhat = yhat.to_numpy()\n",
    "        \n",
    "    # Add the shop and item ids to the forecast so we can merge it with the test csv\n",
    "    yhat_mtx = np.hstack( [ yhat.reshape(len(yhat),1), X_test[['shop_id', 'item_id']].to_numpy() ] )\n",
    "    df_fcst = pd.DataFrame( yhat_mtx, columns=['item_cnt_month', 'shop_id', 'item_id'] )\n",
    "\n",
    "    # Merge the forecast with the test csv\n",
    "    df_out = csv_data['test'].merge( df_fcst, on=['shop_id', 'item_id'], suffixes=('_test', '_forecast'), how='left' )\n",
    "    df_out = df_out[['ID', 'item_cnt_month']]\n",
    "\n",
    "    # Write the combined csv to file\n",
    "    df_out.to_csv( output_file, index=False )\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def get_results_file_name(model_name, idx, data_sets):\n",
    "    \n",
    "    if len(data_sets) == 1 + idx:\n",
    "        output_folder = 'test'\n",
    "    else:\n",
    "        output_folder = 'validation_{}'.format(idx)\n",
    "        \n",
    "    output_file = os.path.join( PROJECT_PATH, 'results', output_folder, model_name + '.csv')    \n",
    "    \n",
    "    return(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "def fit_model( base_model_constructor, model_name, data_sets, \\\n",
    "              feature_names=[], forecast_range=(-np.inf, +np.inf) ):\n",
    "\n",
    "    fitted_models = []\n",
    "    for idx in range(len(data_sets)):\n",
    "\n",
    "        model = base_model_constructor()\n",
    "        X_train_full, y_train, X_test_full, y_test = data_sets[idx]\n",
    "\n",
    "        if len(feature_names) == 0:\n",
    "            X_train = X_train_full\n",
    "            X_test = X_test_full\n",
    "        else:\n",
    "            X_train = X_train_full[feature_names]\n",
    "            X_test = X_test_full[feature_names]\n",
    "        \n",
    "        # Fit the model on the train data and make a prediction on the test set\n",
    "        model.fit( X_train, y_train )\n",
    "        y_hat = model.predict(X_test)\n",
    "        \n",
    "        # Clip forecasts to be in the desired range\n",
    "        y_hat = np.maximum( forecast_range[0], np.minimum( forecast_range[1], y_hat ) )\n",
    "\n",
    "        # Calculate the RMSE\n",
    "        if idx == len(data_sets) - 1:\n",
    "            rmse = np.nan\n",
    "        else:\n",
    "            rmse = np.sqrt( mean_squared_error( y_hat, y_test ) )\n",
    "            \n",
    "        print( 'Writing results for data set {}. RMSE is {}'.format(idx, rmse ) )\n",
    "\n",
    "        # Write the output to file\n",
    "        output_file = get_results_file_name(model_name, idx, data_sets)        \n",
    "        write_forecast_to_file( y_hat, X_test_full, output_file )\n",
    "        \n",
    "        fitted_models.append(model)\n",
    "        \n",
    "    return fitted_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 0. RMSE is 0.7887855760830847\n",
      "Writing results for data set 1. RMSE is 0.8457239282536244\n",
      "Writing results for data set 2. RMSE is 4.165847882776679\n",
      "Writing results for data set 3. RMSE is 3.363813572953488\n",
      "Writing results for data set 4. RMSE is nan\n"
     ]
    }
   ],
   "source": [
    "model_name = 'linreg_01'\n",
    "base_model = lambda : LinearRegression()\n",
    "fitted_models = fit_model( base_model, model_name, data_sets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 0. RMSE is 0.7615322699064294\n",
      "Writing results for data set 1. RMSE is 0.8412637628452622\n",
      "Writing results for data set 2. RMSE is 4.163945612603016\n",
      "Writing results for data set 3. RMSE is 3.3460046410851687\n",
      "Writing results for data set 4. RMSE is nan\n"
     ]
    }
   ],
   "source": [
    "model_name = 'linreg_02'\n",
    "base_model = lambda : LinearRegression()\n",
    "fitted_models = fit_model( base_model, model_name, data_sets, forecast_range=(0,500) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmark():\n",
    "    \n",
    "    def __init__(self, feature_name ):\n",
    "        self.feature_name = feature_name\n",
    "        \n",
    "    def fit(self, X, y ):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        yhat = np.maximum( 0, np.minimum( 20, X[self.feature_name] ) )\n",
    "        return( yhat )\n",
    "    \n",
    "class LinReg():\n",
    "        \n",
    "    def fit(self, X, y ):\n",
    "        self.model = LinearRegression()\n",
    "        #model = Ridge(alpha=0.2, normalize=True)\n",
    "        self.model.fit( X, y )\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Run the model on the validation set and see the score\n",
    "        yhat = self.model.predict( X )\n",
    "        yhat = np.minimum( 20, np.maximum(0, yhat) )       \n",
    "        \n",
    "        # Set the expected sales equal to 0 for shop/items that have not been seen before\n",
    "        idx = X['months_since_first_sale'] == 999\n",
    "        yhat[idx] = 0.3\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 0. RMSE is 1.2838308811187744\n",
      "Writing results for data set 1. RMSE is 1.2979270219802856\n",
      "Writing results for data set 2. RMSE is 4.297789573669434\n",
      "Writing results for data set 3. RMSE is 3.5371546745300293\n",
      "Writing results for data set 4. RMSE is nan\n"
     ]
    }
   ],
   "source": [
    "model_name = 'benchmark'\n",
    "base_model = lambda : Benchmark( 'sales_lag_01')\n",
    "fitted_models = fit_model( base_model, model_name, data_sets, forecast_range=(0,20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 0. RMSE is 1.2713474990173441\n",
      "Writing results for data set 1. RMSE is 1.2771579664777808\n",
      "Writing results for data set 2. RMSE is 4.3283066851969325\n",
      "Writing results for data set 3. RMSE is 3.5152167824381078\n",
      "Writing results for data set 4. RMSE is nan\n"
     ]
    }
   ],
   "source": [
    "model_name = 'linreg_03'\n",
    "base_model = lambda : LinReg()\n",
    "fitted_models = fit_model( base_model, model_name, data_sets, forecast_range=(0,20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25564894"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean( fitted_models[-1].predict( data_sets[-1][-2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=3, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "X_train, y_train, X_test, y_test = data_sets[0]\n",
    "target_features = X_train.columns[:-5]\n",
    "\n",
    "nmf = NMF(n_components=3)\n",
    "nmf.fit(X_train[target_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 2.42 s, total: 2min 49s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "X_train, y_train, X_test, y_test = data_sets[0]\n",
    "\n",
    "model = NearestNeighbors(n_neighbors=3, n_jobs=8)\n",
    "N = 500000\n",
    "\n",
    "x_nmf_train = nmf.transform(X_train[target_features])[:N,:]\n",
    "x_nmf_test = nmf.transform(X_test[target_features])[:N,:]\n",
    "\n",
    "model.fit(x_nmf_train, y_train)\n",
    "#y_hat_0 = model.\n",
    "\n",
    "#rmse = np.sqrt( mean_squared_error( y_hat_0, y_test ))\n",
    "#print( 'RMSE: {}'.format(rmse) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model_name = 'knn_01'\n",
    "feature_names = X_train.columns[1:-6]\n",
    "\n",
    "base_model = lambda : NearestNeighbors(n_neighbors=5)\n",
    "fitted_models = fit_model( base_model, model_name, data_sets, feature_names=feature_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmk_1 = pd.read_csv( os.path.join( PROJECT_PATH, 'results/test/benchmark.csv' ) )\n",
    "bmk_0 = pd.read_csv( os.path.join( PROJECT_PATH, 'forecasts/test/benchmark.csv' ) )\n",
    "bmk_0.sort_values('ID', inplace=True)\n",
    "\n",
    "out = bmk_0.merge(bmk_1, on='ID', suffixes=('_0', '_1'))\n",
    "out.columns = [ 'ID', 'old', 'new']\n",
    "\n",
    "np.corrcoef( out['old'], out['new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                107099.50000\n",
       "item_cnt_month         0.18409\n",
       "dtype: float64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum( ma_ts.to_numpy() > 0, axis=1 ) / np.sum( ts_sales.to_numpy(), axis=1 )[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "ctr = collections.Counter( bmk_0['item_cnt_month'] )\n",
    "ctr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales_cols = ['sales_lag_01', 'sales_lag_02', 'sales_lag_03', 'sales_lag_04', 'sales_lag_05', 'sales_lag_06', 'sales_lag_09', 'sales_lag_12' ]\n",
    "for L in range(len(data_sets)):\n",
    "    X_train, _, _, _ = data_sets[L]\n",
    "    for col in sales_cols:\n",
    "        ctr = collections.Counter(X_train[col])\n",
    "        N = X_train.shape[0]\n",
    "        cnt_vals = np.array([ np.log(ctr[x]/N) for x in range(19) ])\n",
    "        x_vals = np.arange(0,19)\n",
    "        plt.plot( x_vals, cnt_vals )\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vals = np.array([ np.log( np.log(ctr[x]) ) for x in range(19) ])\n",
    "x_vals = np.arange(0,19)\n",
    "plt.plot( x_vals, cnt_vals )\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit( x_vals[:,np.newaxis], cnt_vals[:,np.newaxis] )\n",
    "alpha = model.intercept_[0]\n",
    "beta = model.coef_[0][0]\n",
    "y_hat = alpha + beta * x_vals\n",
    "plt.plot( x_vals, y_hat )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_rmse( model_name, data_sets )\n",
    "\n",
    "    rmse = []\n",
    "    for idx in range(len(data_sets) - 1):\n",
    "\n",
    "        output_file = get_results_file_name(model_name, idx, data_sets)\n",
    "        y_hat = pd.read_csv(output_file)\n",
    "        _, _, X_test, y_test = data_sets[idx]\n",
    "        test = pd.concat( [ X_test.reset_index(drop=True), pd.DataFrame( y_test, columns=['TARGET']) ], axis=1 )\n",
    "\n",
    "        joined = y_hat.merge( test, on='ID' )\n",
    "\n",
    "        rmse.append( np.sqrt( mean_squared_error( joined['TARGET'], joined['item_cnt_month'] ) ) )\n",
    "        \n",
    "        return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = []\n",
    "for j in range(len(data_sets)-1):\n",
    "    X_train, y_train, X_test, y_test = data_sets[j]\n",
    "    rmse.append( np.sqrt(mean_squared_error( np.minimum( 20, X_train['sales_lag_01'] ), y_train ) ) )\n",
    "\n",
    "print(rmse)\n",
    "print(np.mean(rmse))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the benchmark forecast\n",
    "###################################\n",
    "\n",
    "###################################################\n",
    "# First use the validation data set\n",
    "###################################################\n",
    "X_train, y_train, X_test, y_test = data_sets[-2]\n",
    "\n",
    "# Use the previous month's sales as the benchmark forecast\n",
    "yhat = X_test['sales_lag_01']\n",
    "\n",
    "# Cap the forecast values at 20, and floor it at 0\n",
    "yhat = np.maximum( np.minimum( yhat, 20 ), 0 )\n",
    "\n",
    "rmse = np.sqrt( mean_squared_error( yhat, y_test ) )\n",
    "print( 'RMSE on the validation set: {}'.format(rmse))\n",
    "\n",
    "###################################################\n",
    "# Now find the forecast for the test set\n",
    "###################################################\n",
    "\n",
    "# Get the last set of features before the test period\n",
    "_, _, X_test, _ = data_sets[-1]\n",
    "\n",
    "# Use the previous month's sales as the benchmark forecast\n",
    "yhat = X_test['sales_lag_01']\n",
    "\n",
    "# Cap the forecast values at 20, and floor it at 0\n",
    "yhat = np.maximum( np.minimum( yhat, 20 ), 0 )\n",
    "\n",
    "# Write the data to file\n",
    "#output_file = '../forecasts/test/benchmark.csv'\n",
    "#df_out = write_forecast_to_file( yhat, X_test, output_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a forecast with linear regression\n",
    "###################################################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "###################################################\n",
    "# First use the validation data set\n",
    "###################################################\n",
    "X_train, y_train, X_test, y_test = data_sets[-2]\n",
    "\n",
    "# Fit the model using th\n",
    "model = LinearRegression()\n",
    "#model = Ridge(alpha=0.2, normalize=True)\n",
    "model.fit( X_train, y_train )\n",
    "\n",
    "# Run the model on the validation set and see the score\n",
    "yhat = model.predict( X_test )\n",
    "yhat = np.minimum( 20, np.maximum(0, yhat) )\n",
    "\n",
    "rmse = np.sqrt( mean_squared_error( yhat, y_test ) )\n",
    "print( 'RMSE on the validation set: {}'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a forecast with Ridge regression\n",
    "###################################################\n",
    "\n",
    "###################################################\n",
    "# First use the validation data set\n",
    "###################################################\n",
    "X_train, y_train, X_test, y_test = data_sets[-2]\n",
    "\n",
    "# Fit the model using th\n",
    "model = Ridge(alpha=2, normalize=True)\n",
    "model.fit( X_train, y_train )\n",
    "\n",
    "# Run the model on the validation set and see the score\n",
    "yhat = model.predict( X_test )\n",
    "yhat = np.minimum( 20, np.maximum(0, yhat) )\n",
    "\n",
    "rmse = np.sqrt( mean_squared_error( yhat, y_test ) )\n",
    "print( 'RMSE on the validation set: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a forecast with LightGBM\n",
    "###################################################\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "class LGBM():\n",
    "    \n",
    "    def __init__(self, params, categorical_feature ):\n",
    "        self.params = params\n",
    "        self.categorical_feature = categorical_feature\n",
    "        self.model = []\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_test=None, y_test=None ):\n",
    "        lgb_train = lgb.Dataset(X_train, label=y_train.ravel(), \\\n",
    "                            categorical_feature=self.categorical_feature )\n",
    "        \n",
    "        if y_test is None or np.all( np.isnan(y_test) ):\n",
    "            # If there is no validation set, then just fit the model\n",
    "            self.model = lgb.train(params, lgb_train, num_boost_round=20 )\n",
    "        else:\n",
    "            # Define the validation set\n",
    "            lgb_eval = lgb.Dataset(X_test, label=y_test.ravel(), \\\n",
    "                            categorical_feature=self.categorical_feature, reference=lgb_train )\n",
    "    \n",
    "            # Fit the model with the validation set and early stopping\n",
    "            self.model = lgb.train(params, lgb_train, num_boost_round=20,\n",
    "                valid_sets=lgb_eval, early_stopping_rounds=5 )\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_hat = self.model.predict( X, num_iteration=gbm.best_iteration)\n",
    "        return( y_hat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/anaconda3/envs/python37/lib/python3.7/site-packages/lightgbm/basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/Users/chris/anaconda3/envs/python37/lib/python3.7/site-packages/lightgbm/basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's rmse: 3.55662\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's rmse: 3.52179\n",
      "[3]\tvalid_0's rmse: 3.49066\n",
      "[4]\tvalid_0's rmse: 3.46234\n",
      "[5]\tvalid_0's rmse: 3.43875\n",
      "[6]\tvalid_0's rmse: 3.4106\n",
      "[7]\tvalid_0's rmse: 3.38847\n",
      "[8]\tvalid_0's rmse: 3.36681\n",
      "[9]\tvalid_0's rmse: 3.3488\n",
      "[10]\tvalid_0's rmse: 3.33003\n",
      "[11]\tvalid_0's rmse: 3.31067\n",
      "[12]\tvalid_0's rmse: 3.293\n",
      "[13]\tvalid_0's rmse: 3.28196\n",
      "[14]\tvalid_0's rmse: 3.27193\n",
      "[15]\tvalid_0's rmse: 3.25503\n",
      "[16]\tvalid_0's rmse: 3.24482\n",
      "[17]\tvalid_0's rmse: 3.23585\n",
      "[18]\tvalid_0's rmse: 3.22909\n",
      "[19]\tvalid_0's rmse: 3.22107\n",
      "[20]\tvalid_0's rmse: 3.22176\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[19]\tvalid_0's rmse: 3.22107\n",
      "The rmse of prediction is: 3.188448785572161\n"
     ]
    }
   ],
   "source": [
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'rmse', 'rmse'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "X_train, y_train, X_test, y_test = data_sets[-2]\n",
    "model = LGBM(params=params, categorical_feature=cat_features )\n",
    "\n",
    "model.fit(X_train, y_train, X_test, y_test)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_hat) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/anaconda3/envs/python37/lib/python3.7/site-packages/lightgbm/basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 0. RMSE is 0.8915620991031055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/anaconda3/envs/python37/lib/python3.7/site-packages/lightgbm/basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 1. RMSE is 1.0301535214194166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/anaconda3/envs/python37/lib/python3.7/site-packages/lightgbm/basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 2. RMSE is 4.212827720482981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/anaconda3/envs/python37/lib/python3.7/site-packages/lightgbm/basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 3. RMSE is 3.188448785572161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/anaconda3/envs/python37/lib/python3.7/site-packages/lightgbm/basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 4. RMSE is nan\n"
     ]
    }
   ],
   "source": [
    "model_name = 'lightgbm_01'\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'rmse', 'rmse'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "base_model = lambda : LGBM(params=params, categorical_feature=cat_features )\n",
    "fitted_models = fit_model( base_model, model_name, data_sets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a forecast with XGBoost\n",
    "###################################################\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "###################################################\n",
    "# First use the validation data set\n",
    "###################################################\n",
    "X_train, y_train, X_test, y_test = data_sets[-2]\n",
    "\n",
    "n_estimators = 50\n",
    "max_depth = 4           # No lower than 3. Increase until performance stops improving\n",
    "learning_rate = 0.05    # Keep in the range of 0.01 and 0.1\n",
    "gamma = 5               # Regularization parameter: use value 0, 1, or 5\n",
    "colsample_bytree = 0.2  # Between 0.3 and 0.8 when dataset has many columns\n",
    "\n",
    "# Construct the model\n",
    "model = XGBRegressor( n_estimators=n_estimators, \\\n",
    "                      gamma=gamma, \\\n",
    "                      colsample_bytree=colsample_bytree,\\\n",
    "                      max_depth=max_depth, \\\n",
    "                      learning_rate=learning_rate )\n",
    "\n",
    "model.fit( X_train, y_train )\n",
    "\n",
    "# Check the out-of-sample fit for the validation set\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# Print out the RMSE\n",
    "rmse = np.sqrt(mean_squared_error( yhat, y_test ))\n",
    "print( 'RMSE on the validation set: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try a forecast with XGBoost\n",
    "###################################################\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "###################################################\n",
    "# First use the validation data set\n",
    "###################################################\n",
    "X_train, y_train, X_test, y_test = data_sets[-1]\n",
    "\n",
    "n_estimators = 50\n",
    "max_depth = 4           # No lower than 3. Increase until performance stops improving\n",
    "learning_rate = 0.05    # Keep in the range of 0.01 and 0.1\n",
    "gamma = 5               # Regularization parameter: use value 0, 1, or 5\n",
    "colsample_bytree = 0.2  # Between 0.3 and 0.8 when dataset has many columns\n",
    "\n",
    "# Construct the model\n",
    "model = XGBRegressor( n_estimators=n_estimators, \\\n",
    "                      gamma=gamma, \\\n",
    "                      colsample_bytree=colsample_bytree,\\\n",
    "                      max_depth=max_depth, \\\n",
    "                      learning_rate=learning_rate )\n",
    "\n",
    "model.fit( X_train, y_train )\n",
    "\n",
    "# Check the out-of-sample fit for the validation set\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "output_file = '../forecasts/test/xgboost_01.csv'\n",
    "df_out = write_forecast_to_file( yhat, X_test, output_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a forecast with Catboost\n",
    "###################################################\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "###################################################\n",
    "# First use the validation data set\n",
    "###################################################\n",
    "X_train, y_train, X_test, y_test = data_sets[-2]\n",
    "\n",
    "train_data = Pool( X_train, y_train.ravel(), cat_features=cat_features )\n",
    "test_data = Pool( X_test, y_test.ravel(), cat_features=cat_features )\n",
    "\n",
    "# Construct the model\n",
    "model = CatBoostRegressor(\n",
    "    random_seed=63,\n",
    "    iterations=400,\n",
    "    learning_rate=0.05,\n",
    "    l2_leaf_reg=100,\n",
    "    random_strength=1,\n",
    "    one_hot_max_size=4,\n",
    "    leaf_estimation_method='Newton',\n",
    "    eval_metric='RMSE',\n",
    "    depth=10,\n",
    "    boosting_type='Plain',\n",
    "    bootstrap_type='Bernoulli',\n",
    "    subsample=0.2,\n",
    "    rsm=0.2,    \n",
    "    leaf_estimation_iterations=1,\n",
    "    max_ctr_complexity=4,\n",
    "    border_count=32    \n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    logging_level='Silent',\n",
    "    eval_set=test_data,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "# Check the out-of-sample fit for the validation set\n",
    "yhat = model.predict(test_data)\n",
    "\n",
    "# Print out the RMSE\n",
    "rmse = np.sqrt(mean_squared_error( yhat, y_test ))\n",
    "print( 'RMSE on the validation set: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast on the test set with Catboost\n",
    "###################################################\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "###################################################\n",
    "# First use the validation data set\n",
    "###################################################\n",
    "X_train, y_train, X_test, y_test = data_sets[-1]\n",
    "\n",
    "train_data = Pool( X_train, y_train.ravel(), cat_features=cat_features )\n",
    "\n",
    "# Construct the model\n",
    "model = CatBoostRegressor(\n",
    "    random_seed=63,\n",
    "    iterations=400,\n",
    "    learning_rate=0.05,\n",
    "    l2_leaf_reg=100,\n",
    "    random_strength=1,\n",
    "    one_hot_max_size=4,\n",
    "    leaf_estimation_method='Newton',\n",
    "    eval_metric='RMSE',\n",
    "    depth=10,\n",
    "    boosting_type='Plain',\n",
    "    bootstrap_type='Bernoulli',\n",
    "    subsample=0.2,\n",
    "    rsm=0.2,    \n",
    "    leaf_estimation_iterations=1,\n",
    "    max_ctr_complexity=4,\n",
    "    border_count=32    \n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    logging_level='Silent',\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "# Check the out-of-sample fit for the validation set\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "output_file = '../forecasts/test/catboost_05.csv'\n",
    "df_out = write_forecast_to_file( yhat, X_test, output_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python37)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
