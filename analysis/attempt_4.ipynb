{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all csv file data\n",
    "\n",
    "sales = pd.read_csv( '../input/sales_train.csv')\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "item_cat = pd.read_csv( '../input/item_categories.csv')\n",
    "item = pd.read_csv( '../input/items.csv')\n",
    "sub = pd.read_csv( '../input/sample_submission.csv')\n",
    "shops = pd.read_csv( '../input/shops.csv')\n",
    "test = pd.read_csv( '../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the date column\n",
    "sales.date = sales.date.apply( lambda x: datetime.datetime.strptime( x, '%d.%m.%Y' ) )\n",
    "\n",
    "# Add month and year columns\n",
    "sales['month'] = [ x.month for x in sales.date ]\n",
    "sales['year'] = [ x.year for x in sales.date ]\n",
    "sales['year_month'] = sales.year * 100 + sales.month\n",
    "\n",
    "# Add the item_category_id to the training set\n",
    "sales = sales.set_index('item_id').join(item.set_index('item_id')).drop('item_name', axis=1).reset_index()\n",
    "test = test.set_index('item_id').join(item.set_index('item_id')).drop('item_name', axis=1).reset_index()\n",
    "\n",
    "# Add a unique id for the shop + item combo\n",
    "sales['shop_item_id'] = sales.shop_id + sales.item_id * 100\n",
    "test['shop_item_id'] = test.shop_id + test.item_id * 100\n",
    "sales['shop_cat_id'] = sales.shop_id + sales.item_category_id * 100\n",
    "test['shop_cat_id'] = test.shop_id + test.item_category_id * 100\n",
    "\n",
    "# Add the revenue\n",
    "sales[ \"revenue\" ] = sales.item_price * sales.item_cnt_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_rules = {'item_price' : \"mean\", \"revenue\" : \"sum\", \"item_cnt_day\" : \"sum\" }\n",
    "monthly_shop_item = sales.groupby([ \"year_month\", 'date_block_num', \"shop_item_id\", \"item_id\", \"shop_id\", \"item_category_id\" ] ).agg( agg_rules ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_pivot_ts( input_table, pivot_column, val_column, agg_rule, missing_method ):\n",
    "    \n",
    "        # Make time series out of the monthly  sales\n",
    "        tmp_table = input_table.groupby( [ 'year_month', pivot_column ] ).agg( { val_column : agg_rule } ).reset_index()\n",
    "        ts = tmp_table.pivot_table( val_column, index=\"year_month\", columns=pivot_column )\n",
    "        \n",
    "        # Fill missing values with 0\n",
    "        if missing_method == \"zero\":\n",
    "            ts = ts.fillna(0)\n",
    "        elif missing_method == 'ffill':\n",
    "            ts = ts.fillna(method=missing_method)\n",
    "        else:\n",
    "            ValueError( 'Unsupported value: .' + missing_method )\n",
    "        \n",
    "        # Set negative values to 0\n",
    "        ts[ ts < 0 ] = 0\n",
    "        \n",
    "        # Set the index to be the dates\n",
    "        dates = year_month_to_datetime(ts.index.values )\n",
    "        ts = ts.set_index( pd.Index( dates ) )\n",
    "    \n",
    "        # Make sure the dates are sorted\n",
    "        ts.sort_index(axis=0, ascending=True, inplace=True )\n",
    "        \n",
    "        return(ts)\n",
    "    \n",
    "\n",
    "# Define a helper function\n",
    "def year_month_to_datetime( ym ):\n",
    "\n",
    "    if isinstance(ym, float ) or isinstance(ym, int):\n",
    "        m = ym % 100\n",
    "        y = ym // 100\n",
    "\n",
    "        output = datetime.date( y, m, 1 )\n",
    "    else:\n",
    "        if isinstance( ym, pd.Series):\n",
    "            ym = list(ym)\n",
    "\n",
    "        output = []\n",
    "        for j in range(len(ym)):\n",
    "            m = ym[j] % 100\n",
    "            y = ym[j] // 100\n",
    "\n",
    "            output.append( datetime.date( y, m, 1 ) )\n",
    "\n",
    "    return output    \n",
    "\n",
    "def rmse( x1, x2 ):\n",
    "    \n",
    "    res = np.sqrt( np.mean( (x1.ravel()[:,np.newaxis] - x2.ravel()[:,np.newaxis] ) ** 2 ) )\n",
    "    return(res)    \n",
    "\n",
    "\n",
    "def decompose_shop_item_id( shop_item_id ):\n",
    "    \n",
    "    item_id = shop_item_id // 100\n",
    "    shop_id = shop_item_id % 100\n",
    "\n",
    "    return shop_id, item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series of the variables that we will use for prediction\n",
    "ts_item_day = create_pivot_ts( monthly_shop_item, \"shop_item_id\", \"item_cnt_day\", \"sum\", 'zero'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series of the shop id and category id for the different shop/item combinations\n",
    "\n",
    "shop_id, item_id = decompose_shop_item_id( ts_item_day.columns )\n",
    "\n",
    "ts_shop_id = pd.DataFrame( np.vstack( [ shop_id.values ] * ts_item_day.shape[0] ), index=ts_item_day.index )\n",
    "ts_shop_id.columns = ts_item_day.columns\n",
    "\n",
    "uniq_items = item_id.unique()\n",
    "cat_ids_for_uniq_ids = [ item[ item.item_id == x ].item_category_id.iloc[0] for x in uniq_items ]\n",
    "id_map = dict(zip( list(uniq_items), cat_ids_for_uniq_ids ))\n",
    "cat_ids = pd.Series( [ id_map[x] for x in item_id ] )\n",
    "\n",
    "ts_cat_id = pd.DataFrame( np.vstack( [ cat_ids.values ] * ts_item_day.shape[0] ), index=ts_item_day.index )\n",
    "ts_cat_id.columns = ts_item_day.columns\n",
    "\n",
    "date_block_nums = np.array(list(range(0,ts_shop_id.shape[0])))[:,np.newaxis]\n",
    "ts_date_num_block = pd.DataFrame( np.hstack( [ date_block_nums ] * ts_item_day.shape[1] ), index=ts_item_day.index )\n",
    "ts_date_num_block.columns = ts_item_day.columns\n",
    "\n",
    "months = np.array([ x % 100 for x in sorted(sales.year_month.unique()) ] )[:,np.newaxis]\n",
    "ts_month = pd.DataFrame( np.hstack( [ months ] * ts_item_day.shape[1] ), index=ts_item_day.index )\n",
    "ts_month.columns = ts_item_day.columns\n",
    "\n",
    "years = np.array([ x // 100 for x in sorted(sales.year_month.unique()) ] )[:,np.newaxis]\n",
    "ts_year = pd.DataFrame( np.hstack( [ years ] * ts_item_day.shape[1] ), index=ts_item_day.index )\n",
    "ts_year.columns = ts_item_day.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse.csr\n",
    "\n",
    "def convert_to_sparse( xx_input ):\n",
    "\n",
    "    xx_output = dict()\n",
    "    xx_output[DESC] = xx_input[DESC].copy()\n",
    "    \n",
    "    for ds in [ TRAIN, VALID, TEST ]:\n",
    "        xx_output[ds] = scipy.sparse.csr.csr_matrix( xx_input[ds] )    \n",
    "        \n",
    "    return(xx_output)\n",
    "    \n",
    "\n",
    "def add_one_hot_encoding( xx_input, feature_name, binarizer ):\n",
    "\n",
    "    xx_output = dict()\n",
    "    for ds in [ TRAIN, VALID, TEST, DESC ]:\n",
    "        xx_output[ds] = xx_input[ds].copy()\n",
    "    \n",
    "    # Get the column with the target feature\n",
    "    idx = xx_input[DESC].index(feature_name)\n",
    "    \n",
    "    # Update the descriptions of the features\n",
    "    new_cols = [ feature_name + \"_{:02d}\".format(x) for x in binarizer.classes_ ]    \n",
    "    xx_output[DESC] = xx_input[DESC][:idx] + xx_input[DESC][idx+1:] + new_cols\n",
    "    \n",
    "    # (1) Remove the target column from the data sets\n",
    "    # (2) One-hot encode it\n",
    "    # (3) Append to the right side of the data sets\n",
    "    for ds in [ TRAIN, VALID, TEST ]:\n",
    "        target_data = xx_input[ds][:,idx]\n",
    "        one_hot_train = binarizer.transform( target_data.todense() )\n",
    "        tmp = scipy.sparse.hstack( [ xx_input[ds][:,:idx], xx_input[ds][:,idx+1:], one_hot_train ] )\n",
    "        xx_output[ds] = scipy.sparse.csr.csr_matrix( tmp )\n",
    "        \n",
    "    return xx_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the labels and features\n",
    "X = { TRAIN : [], VALID : [], TEST : [], DESC : [] }\n",
    "Y = { TRAIN : [], VALID : [], TEST : [] }\n",
    "\n",
    "# Create the test and train sets from the observation matrices\n",
    "Y = get_regression_vectors_from_matrix( Y, ts_item_day, is_X=False )\n",
    "\n",
    "# Use different sales lags as features\n",
    "lags = [1, 2, 3, 6, 12 ]\n",
    "for L in lags:\n",
    "    x_lag_mtx = ts_item_day.shift(periods=L-1)\n",
    "    X = get_regression_vectors_from_matrix( X, x_lag_mtx, is_X=True, descrip='sales_lag_{:02}'.format(L) )\n",
    "\n",
    "# Use different means as features\n",
    "means = [2, 3, 6, 12]\n",
    "for M in means:\n",
    "    x_mean_mtx = ts_item_day.rolling(window=M).mean()    \n",
    "    X = get_regression_vectors_from_matrix( X, x_mean_mtx, is_X=True, descrip='sales_mean_{:02}'.format(M) )\n",
    "\n",
    "# Add the shop id as a feature\n",
    "X = get_regression_vectors_from_matrix( X, ts_shop_id, is_X=True, descrip='shop_id' )\n",
    "\n",
    "# Add the category id as a feature\n",
    "X = get_regression_vectors_from_matrix( X, ts_cat_id, is_X=True, descrip='cat_id' )\n",
    "\n",
    "# Add the date block number as a feature\n",
    "X = get_regression_vectors_from_matrix( X, ts_date_num_block, is_X=True, descrip='date_block_num' )\n",
    "\n",
    "# Add the month as a feature\n",
    "X = get_regression_vectors_from_matrix( X, ts_month, is_X=True, descrip='month' )\n",
    "\n",
    "# Add the date block number as a feature\n",
    "X = get_regression_vectors_from_matrix( X, ts_year, is_X=True, descrip='year' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the combined features in matrix form for the train, validation and test sets\n",
    "xx_raw, yy = combine_features( X, Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the features (e.g. winsorize, clip values, etc.)\n",
    "xx = preprocess_features(xx_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_sparse = convert_to_sparse( xx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace shop_id with one-hot encodings\n",
    "shop_binarizer = LabelBinarizer(sparse_output=True)\n",
    "shop_binarizer.fit(sales.shop_id.unique())\n",
    "xx_sparse = add_one_hot_encoding( xx_sparse, \"shop_id\", binarizer=shop_binarizer )\n",
    "\n",
    "# Replace category_id with one-hot encodings\n",
    "cat_binarizer = LabelBinarizer(sparse_output=True)\n",
    "cat_binarizer.fit(sales.item_category_id.unique())\n",
    "xx_sparse = add_one_hot_encoding( xx_sparse, \"cat_id\", binarizer=shop_binarizer )\n",
    "\n",
    "# Replace month with one-hot encodings\n",
    "cat_binarizer = LabelBinarizer(sparse_output=True)\n",
    "cat_binarizer.fit(np.arange(1,12))\n",
    "xx_sparse = add_one_hot_encoding( xx_sparse, \"month\", binarizer=shop_binarizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_sklearn( model_constructor_fun, xx_input, yy_input, output_file, \\\n",
    "                                     date_block_cutoff=0, clip_forecasts=(0,20)):\n",
    "    \"\"\"A function that performs the validation testing and writes the formatted\n",
    "    predictions to a csv file. \n",
    "    \n",
    "    The forecasts will be based on the combined train and validation sets.\n",
    "    \n",
    "    The model_constructor_fun should take no arguments, and produce a model of the sklearn class.\n",
    "    The model that is created is expected to have both 'fit' and 'predict' methods.\n",
    "    \n",
    "    The results of the tests will be written to the output_file.\"\"\"\n",
    "    \n",
    "    # Exclude date blocks before the cutoff\n",
    "    if date_block_cutoff > 0:\n",
    "        idx_db_col = [ x == 'date_block_num' for x in xx_input[DESC] ]\n",
    "        \n",
    "        # Make copies before removing rows so we don't change the original matrices\n",
    "        xx = dict()\n",
    "        yy = dict()        \n",
    "        xx[DESC] = xx_input[DESC].copy()\n",
    "        for ds in [ TRAIN, VALID, TEST ]:\n",
    "            xx[ds] = xx_input[ds].copy()\n",
    "            yy[ds] = yy_input[ds].copy()\n",
    "            \n",
    "            idx = [ r > date_block_cutoff for r in xx[ds][:,idx_db_col] ]\n",
    "            xx[ds] = xx[ds][idx,:]\n",
    "            yy[ds] = yy[ds][idx,:]\n",
    "    else:\n",
    "        # No need to make copies if we are not removing rows\n",
    "        xx = xx_input\n",
    "        yy = yy_input\n",
    "    \n",
    "    # Fit the model using the training data\n",
    "    model = model_constructor_fun()\n",
    "    model.fit( xx[TRAIN], yy[TRAIN] )        \n",
    "\n",
    "    # Run the model on the validation set and see the score\n",
    "    yhat_valid = model.predict(xx[VALID])\n",
    "\n",
    "    # Clip the forecast to lie in the appropriate interval\n",
    "    yhat_valid = np.maximum( clip_forecasts[0], yhat_valid )\n",
    "    yhat_valid = np.minimum( clip_forecasts[1], yhat_valid )    \n",
    "\n",
    "    res = rmse( yhat_valid, yy[VALID] )\n",
    "    print( 'Validation RMSE {}'.format( res ) )\n",
    "\n",
    "    ###############################################################\n",
    "    # Write the forecast for the test set to csv\n",
    "\n",
    "    # Fit the model using the training plus validation data\n",
    "    model = model_constructor_fun()\n",
    "    if isinstance( xx[TRAIN], np.ndarray ):\n",
    "        model.fit( np.vstack( [ xx[TRAIN], xx[VALID] ] ), \\\n",
    "                   np.vstack( [ yy[TRAIN], yy[VALID] ] ) )\n",
    "    else:\n",
    "        model.fit( scipy.sparse.vstack( [ xx[TRAIN], xx[VALID] ] ), \\\n",
    "                   np.vstack( [ yy[TRAIN], yy[VALID] ] ) )        \n",
    "\n",
    "    # Forecast values for the test set\n",
    "    yhat_test = model.predict(xx[TEST])\n",
    "    \n",
    "    # Clip the forecast to lie in the appropriate interval\n",
    "    yhat_test = np.maximum( clip_forecasts[0], yhat_test )\n",
    "    yhat_test = np.minimum( clip_forecasts[1], yhat_test )   \n",
    "    \n",
    "    # Format the forecast as a Pandas Series and write the output to .csv\n",
    "    fcst = pd.Series( yhat_test.ravel(), index=pd.Index( ts_item_day.columns, dtype=\"int64\") )\n",
    "    fcst_df = format_forecast( test, fcst, fill_na=True )\n",
    "\n",
    "    # Write the results to the output file\n",
    "    fcst_df.to_csv( '../forecasts/'+output_file, index=False, header=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE 3.942574988430639\n"
     ]
    }
   ],
   "source": [
    "# Benchmark forecast - use the previous month's sales\n",
    "\n",
    "yhat_valid = xx[VALID][:,0]\n",
    "\n",
    "res = rmse( yhat_valid, yy[VALID] )\n",
    "print( 'Validation RMSE {}'.format( res ) )\n",
    "\n",
    "###############################################################\n",
    "# Write the forecast for the test set to csv\n",
    "\n",
    "yhat_test = xx[TEST][:,0]\n",
    "\n",
    "# Format the forecast as a Pandas Series and write the output to .csv\n",
    "fcst = pd.Series( yhat_test.ravel(), index=pd.Index( ts_item_day.columns, dtype=\"int64\") )\n",
    "fcst_df = format_forecast( test, fcst, fill_na=True )\n",
    "\n",
    "fcst_df.to_csv( '../forecasts/benchmark_04.csv', index=False, header=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "model_constructor_fun = lambda : LinearRegression()\n",
    "forecast_sklearn( model_constructor_fun, xx, yy, 'linreg_02.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "model_constructor_fun = lambda : Ridge(alpha=0.2, normalize=True)\n",
    "forecast_sklearn( model_constructor_fun, xx_sparse, yy, 'ridge_03.csv', date_block_cutoff=32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "model_constructor_fun = lambda : SVR(C=1)\n",
    "forecast_sklearn( model_constructor_fun, xx, yy, 'svr_01.csv', date_block_cutoff=30 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3295\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3296\u001b[0;31m                     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3297\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-137-bb57e71d3d99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_constructor_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mforecast_sklearn\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel_constructor_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ridge_03.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_block_cutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-135-6472b088c32b>\u001b[0m in \u001b[0;36mforecast_sklearn\u001b[0;34m(model_constructor_fun, xx_input, yy_input, output_file, date_block_cutoff)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdate_block_cutoff\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx_db_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-6472b088c32b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdate_block_cutoff\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx_db_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0mi0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mcheck_format\u001b[0;34m(self, full_check)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mprune\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prune_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prune_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.7/site-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m_prune_array\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_prune_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \"\"\"Return an array equivalent to the input array. If the input\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Try to forecast using Random forests\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_constructor_fun = lambda : RandomForestRegressor(n_estimators=10, max_depth=2)\n",
    "forecast_sklearn( model_constructor_fun, xx_sparse, yy, 'ridge_03.csv', date_block_cutoff=32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE 1.965791369627067\n"
     ]
    }
   ],
   "source": [
    "# Try to forecast using Adaboost\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "sub_cols = [0,2,5,11,17]\n",
    "sub_cols = [0,2,5,11,17]\n",
    "xx_train_sub = xx_train[:,sub_cols]\n",
    "xx_valid_sub = xx_valid[:,sub_cols]\n",
    "xx_test_sub = xx_test[:,sub_cols]\n",
    "\n",
    "# Fit the model using the training data\n",
    "BE = DecisionTreeRegressor(max_depth=3)\n",
    "BE = LinearRegression()\n",
    "model = AdaBoostRegressor(n_estimators=50, random_state=0, base_estimator=BE, loss=\"linear\", learning_rate=0.0005 )\n",
    "model.fit(xx_train_sub, yy_train)\n",
    "\n",
    "# Run the model on the validation set and see the score\n",
    "yhat_valid = model.predict(xx_valid_sub).reshape(xx_valid_sub.shape[0],1)\n",
    "yhat_valid = np.maximum(0, yhat_valid)\n",
    "\n",
    "# Check the goodness of fit\n",
    "res = rmse( yhat_valid, yy_valid)\n",
    "print( 'Validation RMSE {}'.format( res ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE 1.7383397340041775\n"
     ]
    }
   ],
   "source": [
    "# Try to forecast using Random forests\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# Fit the model using the training data\n",
    "model = Ridge(alpha=0.6, normalize=True)\n",
    "model.fit(xx_train, yy_train)\n",
    "\n",
    "# Run the model on the validation set and see the score\n",
    "yhat_valid = model.predict(xx_valid).reshape(xx_valid.shape[0],1)\n",
    "yhat_valid = np.maximum(0, yhat_valid)\n",
    "\n",
    "# Check the goodness of fit\n",
    "res = rmse( yhat_valid, yy_valid)\n",
    "print( 'Validation RMSE {}'.format( res ) )\n",
    "\n",
    "# Make a forecast using the test data\n",
    "yhat_test = model.predict(xx_test).reshape(xx_test.shape[0],1)\n",
    "yhat_test = np.maximum(0, yhat_test)\n",
    "\n",
    "# Format the forecast as a Pandas Series and write the output to .csv\n",
    "fcst = pd.Series( yhat_test.ravel(), index=pd.Index(ts_item_day.columns) )\n",
    "output = format_forecast( test.ID, fcst )\n",
    "\n",
    "output.to_csv( '../forecasts/forecast_10.csv', index=True, header=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_month</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_item_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>revenue</th>\n",
       "      <th>item_cnt_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201301</td>\n",
       "      <td>0</td>\n",
       "      <td>1925</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201301</td>\n",
       "      <td>0</td>\n",
       "      <td>2701</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201301</td>\n",
       "      <td>0</td>\n",
       "      <td>2702</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201301</td>\n",
       "      <td>0</td>\n",
       "      <td>2710</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201301</td>\n",
       "      <td>0</td>\n",
       "      <td>2719</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_month  date_block_num  shop_item_id  item_id  shop_id  \\\n",
       "0      201301               0          1925       19       25   \n",
       "1      201301               0          2701       27        1   \n",
       "2      201301               0          2702       27        2   \n",
       "3      201301               0          2710       27       10   \n",
       "4      201301               0          2719       27       19   \n",
       "\n",
       "   item_category_id  item_price  revenue  item_cnt_day  \n",
       "0                40        28.0     28.0           1.0  \n",
       "1                19      1890.0   1890.0           1.0  \n",
       "2                19      2499.0   2499.0           1.0  \n",
       "3                19      1890.0   1890.0           1.0  \n",
       "4                19      2499.0   2499.0           1.0  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_shop_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'price_lag_12'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python37)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
