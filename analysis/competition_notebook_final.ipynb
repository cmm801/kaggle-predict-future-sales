{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The notebook is broken into 5 main sections.\n",
    "\n",
    "**Section 1: Data import and cleaning**<br>\n",
    "Import the data, format it as a pandas data frame, and add a few columns for convenience\n",
    "\n",
    "**Section 2: Feature Engineering**<br>\n",
    "Add the majority of the features in this section.\n",
    "\n",
    "**Section 3: Train/Validation/Test set split**<br>\n",
    "Here, split the data into train/validation/test sets. The validation and test set are always in the future so that we do not assume any ability to see into the future\n",
    "\n",
    "**Section 4: Mean encoding: Use K-fold cross-validation** (like what we did in class)<br>\n",
    "Add mean-encoded categorical variables on the train/validation/test sets\n",
    "\n",
    "**Section 5: Construct Forecast**<br> \n",
    "Create a set of classes that streamline our ability to make forecasts across a range of models. Our final forecast is done using RandomForestRegressor from sklearn.\n",
    "\n",
    "**Appendix: Hyperparameter optimization**<br> \n",
    "Working with RandomForestRegressor class from sklearn, use the hyperopt package to find the set of hyperparameters that gives the best RMSE on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Import and Cleaning\n",
    "\n",
    "**You must set PROJECT_PATH to be the folder containing this notebook. This will be used for reading data and writing output.**\n",
    "\n",
    "The csv files should be in a directory called 'input' in the PROJECT_PATH folder.\n",
    "\n",
    "The code below reads in the csv files, and for each month, adds all shop/item combinations to the training set. \n",
    "\n",
    "We aggregate the data to monthly frequency. In the training data frame, we rename the 'item_cnt_month' to 'TARGET'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b>\n",
    "You must set PROJECT_PATH to be the folder containing this notebook. This will be used for reading data and writing output.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# YOU MUST SET THIS to point to the folder containing this notebook\n",
    "# Define some constants that we will use\n",
    "PROJECT_PATH = '/home/ubuntu/projects/kaggle-predict-future-sales'\n",
    "########################################################################\n",
    "\n",
    "\n",
    "# Can set this to False to use the memory-intensive set of features\n",
    "USE_LOW_MEMORY_VERSION = True\n",
    "\n",
    "TRAIN = 'train'\n",
    "VALID = 'valid'\n",
    "TEST = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import datetime\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import hyperopt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw CSV files\n",
    "\n",
    "csv_data = dict()\n",
    "\n",
    "# Import all csv file data\n",
    "csv_data[ 'sales_daily' ] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/sales_train.csv') )\n",
    "csv_data['item_cat'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/item_categories.csv') )\n",
    "csv_data['item'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/items.csv') )\n",
    "csv_data['sub'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/sample_submission.csv') )\n",
    "csv_data['shops'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/shops.csv') )\n",
    "csv_data['test'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/test.csv') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some new columns and aggregate from daily to monthly\n",
    "############################################################\n",
    "\n",
    "# Add the daily revenue\n",
    "csv_data['sales_daily'][ \"revenue\" ] = csv_data['sales_daily'].item_price * csv_data['sales_daily'].item_cnt_day\n",
    "\n",
    "# Aggregate the monthly data\n",
    "agg_rules = {'item_price' : \"mean\", \"revenue\" : \"sum\", \"item_cnt_day\" : \"sum\" }\n",
    "groupby_cols = [ 'date_block_num', \"item_id\", \"shop_id\" ]\n",
    "\n",
    "# Add the effective item price\n",
    "csv_data['sales_monthly'] = csv_data['sales_daily'].groupby( groupby_cols ).agg( agg_rules ).reset_index()\n",
    "\n",
    "# Rename the column to reflect monthly data\n",
    "csv_data['sales_monthly'].rename( columns={ 'item_cnt_day' : 'item_cnt_month'}, inplace=True )\n",
    "\n",
    "# Add unit price as a column\n",
    "csv_data['sales_monthly']['item_price_unit'] = np.round( csv_data['sales_monthly']['revenue'] / \\\n",
    "                              ( 1e-6 + csv_data['sales_monthly']['item_cnt_month'] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all combinations of shops/items for each date\n",
    "############################################################\n",
    "\n",
    "df_monthly = csv_data['sales_monthly']\n",
    "\n",
    "# Only use last 13 months if we are trying to save memory\n",
    "if USE_LOW_MEMORY_VERSION:\n",
    "    dates = df_monthly['date_block_num'].unique()[-13:]\n",
    "else:\n",
    "    dates = df_monthly['date_block_num'].unique()\n",
    "\n",
    "# Loop through the dates and append each shop/item combination\n",
    "df_id = pd.DataFrame( [], columns=['date_block_num', 'shop_id', 'item_id'])\n",
    "for dt in dates:\n",
    "    df_t = df_monthly[ df_monthly['date_block_num'] == dt ]\n",
    "    uniq_shops = df_t['shop_id'].unique()    \n",
    "    uniq_items = df_t['item_id'].unique()\n",
    "        \n",
    "    new_rows = pd.DataFrame( itertools.product( [dt], uniq_shops, uniq_items ), columns=df_id.columns )\n",
    "    df_id = pd.concat( [ df_id, new_rows ], sort=False, axis=0 )\n",
    "\n",
    "# Join the test IDs to the data frame\n",
    "df_t = csv_data['test'].copy()\n",
    "df_t['date_block_num'] = 1 + df_id['date_block_num'].max()\n",
    "df_id = pd.concat( [ df_id, df_t ], sort=False, axis=0 ).drop(['ID'], axis=1)\n",
    "\n",
    "# Create a data frame using all the shop/item pairs from each month\n",
    "df_sales = df_id.merge( csv_data['sales_monthly'], on=['date_block_num', 'shop_id', 'item_id' ], how='left' )\n",
    "\n",
    "# Set missing values for revenue and item count to 0\n",
    "df_sales.loc[ np.isnan(df_sales['revenue']), ['revenue'] ] = 0\n",
    "df_sales.loc[ np.isnan(df_sales['item_cnt_month']), ['item_cnt_month'] ] = 0\n",
    "\n",
    "# Make sure values are still NaN for the test period ('date_block_num' == 34)\n",
    "df_sales.loc[ df_sales['date_block_num'] == 34, 'revenue' ] = np.nan\n",
    "df_sales.loc[ df_sales['date_block_num'] == 34, 'item_cnt_month' ] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns to the data frame\n",
    "############################################################\n",
    "\n",
    "# Add the category id\n",
    "df_sales = df_sales.merge( csv_data['item'], on='item_id' )\n",
    "df_sales = df_sales.drop('item_name', axis=1 )\n",
    "\n",
    "# Add a column combining shop and item, which together with date_block_num is a unique id\n",
    "df_sales['shop_item_id'] = df_sales['shop_id'] + 100 * df_sales['item_id']\n",
    "\n",
    "# Add the month\n",
    "df_sales['month'] = 1 + (df_sales['date_block_num'] % 12)\n",
    "\n",
    "# Rename the target column and make it the first column\n",
    "target = df_sales.loc[:,['item_cnt_month']]\n",
    "target.columns = [ 'TARGET']\n",
    "df_sales = pd.concat( [ target, df_sales.drop('item_cnt_month', axis=1) ], axis=1 ) \n",
    "\n",
    "# Clip values to be in [0,20]\n",
    "df_sales['TARGET'] = np.clip( df_sales['TARGET'], 0, 20 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Feature Engineering\n",
    "\n",
    "First, create a time series data frame named 'ts_sales', containing monthly sales. The rows are the months, and the columns are unique shop/item pairs.\n",
    "\n",
    "Once we have 'ts_sales', we can extract some useful features like moving averages, standard deviations, quantiles, etc. We only use a few of these to avoid overloading the memory.\n",
    "\n",
    "The time series features calculated from ts_sales are stored as a list of column vectors. Once we have finished calculating new features, these column vectors are all combined into the data frame 'df_calc'.\n",
    "\n",
    "Finally, we join the calculated features with the original data frame 'df_sales', obtained earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.26 s, sys: 485 ms, total: 5.74 s\n",
      "Wall time: 5.46 s\n"
     ]
    }
   ],
   "source": [
    "# Create a grid of the monthly sales and shop/item combinations\n",
    "############################################################################\n",
    "\n",
    "# Make time series out of the monthly  sales\n",
    "pivot_columns = [ 'shop_id', 'item_id']\n",
    "target_col = 'item_cnt_month'\n",
    "grp_table = csv_data['sales_monthly'].groupby( [ 'date_block_num' ] + pivot_columns ).agg( { target_col : 'sum' } ).reset_index()\n",
    "ts_sales_raw = grp_table.pivot_table( target_col, index=\"date_block_num\", columns=pivot_columns )\n",
    "\n",
    "# Clip all values to be in [0,20]\n",
    "ts_sales_raw = np.clip( ts_sales_raw, 0, 20 )\n",
    "\n",
    "# Make sure the dates are sorted\n",
    "ts_sales_raw.sort_index(axis=0, ascending=True, inplace=True )\n",
    "\n",
    "# Create a version that has missing values filled with 0\n",
    "ts_sales = ts_sales_raw.fillna(0)\n",
    "\n",
    "# Downcast both versions\n",
    "ts_sales_raw = ts_sales_raw.astype('float32')\n",
    "ts_sales = ts_sales.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 826 ms, total: 11.4 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "# Calculate most of the features in this block\n",
    "############################################################################\n",
    "\n",
    "# Initialize a list to store features. We will concatenate these afterwards with hstack\n",
    "feature_vals = []\n",
    "feature_names = []\n",
    "\n",
    "# Just keep the last M months of observations, and convert to a numpy column vector\n",
    "process_features = lambda x : x.to_numpy().ravel()[:,np.newaxis]\n",
    "\n",
    "# Add the 'date_block_num' as a feature\n",
    "ts_dates = np.tile( np.array(ts_sales.index)[:,np.newaxis], ts_sales.shape[1] )\n",
    "feature_vals.append( process_features(pd.DataFrame(ts_dates) ) )\n",
    "feature_names.append( 'date_block_num' )\n",
    "\n",
    "# Add 'shop_item_id' as a feature\n",
    "for j, col in enumerate(ts_sales.columns.names):\n",
    "    ts = np.array([ x[j] for x in ts_sales.columns ] * ts_sales.shape[0]).reshape( ts_sales.shape[0], ts_sales.shape[1])\n",
    "    feature_vals.append( process_features(pd.DataFrame(ts) ) )  \n",
    "    feature_names.append( col )\n",
    "    \n",
    "if USE_LOW_MEMORY_VERSION:\n",
    "    # Add 1-month lagged sales\n",
    "    ts = ts_sales.copy()\n",
    "    feature_vals.append( process_features(ts) )\n",
    "    feature_names.append( 'sales_lag_01' )\n",
    "    \n",
    "    # Add 6-month mean rolling sales\n",
    "    ts = ts_sales.rolling(window=6).mean()\n",
    "    feature_vals.append( process_features(ts) )    \n",
    "    feature_names.append( 'sales_mean_06' )\n",
    "    \n",
    "    # Add 12-month mean rolling sales\n",
    "    ts = ts_sales.rolling(window=12).mean()\n",
    "    feature_vals.append( process_features(ts) )    \n",
    "    feature_names.append( 'sales_mean_12' )    \n",
    "else:\n",
    "    # Get lagged monthly sales\n",
    "    print( 'Calculating lagged sales...')\n",
    "    lags = np.arange(1,13)\n",
    "    for L in lags:\n",
    "        ts = ts_sales.shift(periods=L-1)\n",
    "        feature_vals.append( process_features(ts) )\n",
    "        feature_names.append( 'sales_lag_{:02}'.format(L) )\n",
    "\n",
    "    # Get lagged monthly mean sales\n",
    "    print( 'Calculating rolling mean sales...')\n",
    "    means = [3, 6, 9, 12]\n",
    "    for M in means:\n",
    "        ts = ts_sales.rolling(window=M).mean()\n",
    "        feature_vals.append( process_features(ts) )    \n",
    "        feature_names.append( 'sales_mean_{:02}'.format(M) )\n",
    "\n",
    "    # Get 12-month standard deviation\n",
    "    print( 'Calculating std. dev. of sales...')\n",
    "    means = [6, 12]\n",
    "    for M in means:\n",
    "        ts = ts_sales.rolling(window=M).std()\n",
    "        feature_vals.append( process_features(ts) )\n",
    "        feature_names.append( 'sales_std_{:02}'.format(M) )\n",
    "\n",
    "    # Get the 12-month quartiles\n",
    "    print( 'Calculating quantiles of sales...')\n",
    "    for p in [ 0.25, 0.50, 0.75]:\n",
    "        ts = ts_sales.rolling(window=12).quantile(p)\n",
    "        feature_vals.append( process_features(ts) )    \n",
    "        feature_names.append( 'sales_percentile_{:02}_12'.format(int(p * 100) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to Downcast floats and ints to 32-bit to save memory\n",
    "############################################################################\n",
    "\n",
    "def downcast_dataframe( df ):\n",
    "    for col in df.columns:\n",
    "        if isinstance( df[col ].iloc[0], np.float64 ) or isinstance( df[col ].iloc[0], float ):\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif isinstance( df[col ].iloc[0], np.int64 ) or isinstance( df[col ].iloc[0], int ):\n",
    "            df[col] = df[col].astype('int32')\n",
    "            \n",
    "    return df\n",
    "\n",
    "########################################################################################\n",
    "# Function to extract the first/last observation in a time series for each column\n",
    "########################################################################################\n",
    "\n",
    "def get_months_since_first_and_last_observation( input_ts ):\n",
    "    \"\"\"Gets the number of months since the first and last observation in each columns, \n",
    "    at each point in time. This function has no 'look-forward' bias.\"\"\"\n",
    "    \n",
    "    # Create an array with the index (0 to T) of any non-zeros entries\n",
    "    month_of_obs = input_ts.to_numpy() * np.arange(1,input_ts.shape[0]+1)[:,np.newaxis]\n",
    "    month_of_obs = month_of_obs.astype('float32')\n",
    "    month_of_obs[ np.less( month_of_obs, 0.1, where=~np.isnan(month_of_obs)) ] = np.nan\n",
    "\n",
    "    # Make a data frame\n",
    "    df_month_of_obs = pd.DataFrame( month_of_obs, columns=input_ts.columns)\n",
    "\n",
    "    # Loop through the time steps and find the months since first/last action at each t\n",
    "    months_since_first_obs = np.nan * np.ones_like(month_of_obs)\n",
    "    months_since_last_obs = np.nan * np.ones_like(month_of_obs)\n",
    "    for t in range( month_of_obs.shape[0] ):\n",
    "        months_since_first_obs[t,:] = t - df_month_of_obs.iloc[:(t+1),:].idxmin(axis=0)\n",
    "        months_since_last_obs[t,:] = t - df_month_of_obs.iloc[:(t+1),:].idxmax(axis=0)    \n",
    "\n",
    "    # Fill NaN's with 999 for shop/items that have never seen a sale\n",
    "    months_since_first_obs[ np.isnan( months_since_first_obs ) ] = 999\n",
    "    months_since_last_obs[ np.isnan( months_since_last_obs ) ] = 999\n",
    "\n",
    "    # Convert back into pandas Dataframe\n",
    "    months_since_first_obs = pd.DataFrame( months_since_first_obs, columns=ts_sales.columns )\n",
    "    months_since_last_obs = pd.DataFrame( months_since_last_obs, columns=ts_sales.columns )\n",
    "    \n",
    "    return months_since_first_obs, months_since_last_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 989 ms, total: 1min 5s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "# Downcast the feature values\n",
    "# Get the number of months since the first and last sale, at each point in time\n",
    "months_since_first_sale, months_since_last_sale = \\\n",
    "        get_months_since_first_and_last_observation( ts_sales_raw )\n",
    "\n",
    "# Add to features\n",
    "feature_vals.append( process_features( months_since_first_sale.astype('float32') ) )\n",
    "feature_names.append( 'months_since_first_sale' )\n",
    "feature_vals.append( process_features( months_since_last_sale.astype('float32') ) )\n",
    "feature_names.append( 'months_since_last_sale' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.09 s, sys: 436 ms, total: 1.53 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "# Join the features into a pandas data frame\n",
    "features_list = [ x for x in feature_vals ]\n",
    "df_calc = pd.DataFrame( np.hstack(features_list), columns=feature_names)\n",
    "\n",
    "# We must shift the date_block_num forward by 1 for the calculated columns, \n",
    "#    since these are not available until the next month\n",
    "df_calc['date_block_num'] = 1 + df_calc['date_block_num'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 3.67 s, total: 15.6 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Combine the sales data with the calculated features\n",
    "df_full = df_sales.merge( df_calc, on=['date_block_num', 'shop_id', 'item_id' ], how='left' )\n",
    "df_full = downcast_dataframe(df_full)\n",
    "\n",
    "# Only use the last few periods if we are trying to save memory\n",
    "if USE_LOW_MEMORY_VERSION:\n",
    "    df_full = df_full[ df_full['date_block_num'] >= 31 ]\n",
    "else:\n",
    "    df_full = df_full[ df_full['date_block_num'] >= 22 ]\n",
    "\n",
    "# Clean up missing values in the calculated columns\n",
    "calc_sales_cols = list( df_calc.columns[ [ x.startswith('sales_') for x in df_calc.columns ] ] )\n",
    "df_full[calc_sales_cols] = df_full[calc_sales_cols].fillna(0)\n",
    "\n",
    "# Add features for months since first/last sale of a shop/item pair\n",
    "df_full['months_since_first_sale'] = df_full['months_since_first_sale'].fillna(999).values\n",
    "df_full['months_since_last_sale'] = df_full['months_since_last_sale'].fillna(999).values\n",
    "\n",
    "# Add a binary feature that describes whether a shop/item pair is active\n",
    "df_full['shop_item_never_active'] = ( df_full['months_since_first_sale'] == 999 ).astype('int32')\n",
    "df_full['shop_item_inactive'] = ( df_full['months_since_last_sale'] > 12 ).astype('int32')\n",
    "\n",
    "# Create a new column representing the total sales over the past 12 months\n",
    "df_full['sales_total_12'] = 12 * df_full['sales_mean_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional grouped features \n",
    "#############################################\n",
    "\n",
    "def create_group_features( df, group_col, agg_rule, target_col, new_col_name=None ):\n",
    "    if new_col_name is None:\n",
    "        new_col_name = target_col + '_' + agg_rule + '_by_' + group_col \n",
    "        \n",
    "    vals = df_full.groupby( [ 'date_block_num', group_col ] )[ target_col ].transform(agg_rule)\n",
    "    df[new_col_name] = vals.astype('float32')    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 1.61 s, total: 3.34 s\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "# Add additional categorical features by grouping among different columns \n",
    "##########################################################################################\n",
    "\n",
    "# Create a new column representing the total sales over the past 12 months\n",
    "df_full['sales_total_12'] = 12 * df_full['sales_mean_12']\n",
    "\n",
    "for target_col in [ 'sales_lag_01', 'sales_total_12' ]:\n",
    "    for agg_rule in [ 'sum', 'mean' ]:\n",
    "        for group_col in [ 'item_id', 'shop_id', 'item_category_id' ]:\n",
    "            df_full = create_group_features( df_full, group_col, agg_rule, target_col )\n",
    "\n",
    "for group_col in [ 'item_id', 'shop_id', 'item_category_id']:\n",
    "    df_full = create_group_features( df_full, group_col, 'min', 'months_since_first_sale', \\\n",
    "                                    new_col_name='min_months_since_first_sale_for_' + group_col.replace('_id', '') )\n",
    "    df_full = create_group_features( df_full, group_col, 'min', 'months_since_last_sale', \\\n",
    "                                    new_col_name='min_months_since_last_sale_for_' + group_col.replace('_id', '') )\n",
    "    \n",
    "    df_full = create_group_features( df_full, group_col, 'mean', 'shop_item_never_active', \\\n",
    "                                    new_col_name=group_col.replace('_id', '') +'_mean_never_active' )\n",
    "    df_full = create_group_features( df_full, group_col, 'mean', 'shop_item_inactive', \\\n",
    "                                    new_col_name=group_col.replace('_id', '') +'_mean_inactive' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base set of features that will be used for training and validation\n",
    "########################################################################\n",
    "\n",
    "# Drop the revenue and price columns\n",
    "df_base = df_full.drop( [ 'item_price', 'item_price_unit', 'revenue' ], axis=1 )\n",
    "\n",
    "# Only keep the past 12 periods for testing\n",
    "df_base = downcast_dataframe( df_base )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Train/Validation/Test set split\n",
    "\n",
    "Create the different train/validation/test sets. The methodology is to construct a number of train/validation set splits over different time frames. Within each such split, the validation data comes from the month following the end of the training set.\n",
    "\n",
    "We can construct any number of such triplets (e.g. train/validation/test), but for convenience here we just do this twice. The first group will be the training set + validation set. The second group will be the expanded training data plus the forecast for the actual test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get different datasets, in chronological order\n",
    "# The test set is always the next period after the end of the train set.\n",
    "##########################################################################\n",
    "\n",
    "def get_validation_set( df, idx, min_train_size ):\n",
    "\n",
    "    date_blocks = df['date_block_num']\n",
    "    uniq_date_blocks = np.sort( date_blocks.unique() )\n",
    "\n",
    "    n_datasets = len(uniq_date_blocks) - min_train_size - 1\n",
    "    if idx >= 0:\n",
    "        split_date = uniq_date_blocks[min_train_size + idx]\n",
    "    else:\n",
    "        split_date = uniq_date_blocks[idx]    \n",
    "\n",
    "    xtrain = df[ date_blocks < split_date].drop('TARGET', axis=1 )\n",
    "    ytrain = df[ date_blocks < split_date ]['TARGET'][:,np.newaxis]\n",
    "    xtest = df[ date_blocks == split_date ].drop('TARGET', axis=1 )\n",
    "    ytest = df[ date_blocks == split_date ]['TARGET'][:,np.newaxis]\n",
    "\n",
    "    return xtrain, ytrain, xtest, ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some train/validation/test data sets\n",
    "##########################################################################\n",
    "\n",
    "# Specify which Dataframe to use\n",
    "features_df = df_base\n",
    "\n",
    "if USE_LOW_MEMORY_VERSION:\n",
    "    MIN_TRAIN_SIZE = 2\n",
    "else:\n",
    "    MIN_TRAIN_SIZE = 5\n",
    "\n",
    "data_sets = []\n",
    "for j in range(100):\n",
    "    try:\n",
    "        xtrain, ytrain, xtest, ytest = get_validation_set( features_df, j, min_train_size=MIN_TRAIN_SIZE )\n",
    "        data_sets.append( ( xtrain, ytrain, xtest, ytest ) )\n",
    "    except IndexError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Mean encoding: Use K-fold cross-validation\n",
    "\n",
    "Once we have split data into train/test sets, we can use the K-fold mean-encoding algorithm discussed in class. \n",
    "\n",
    "The mean encoding could not be implemented before splitting the data into train/test sets, as this would have led to data leakages and potentially overfitting.\n",
    "\n",
    "We mean-encode the TARGET values on several of the categorical features below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for mean encoding\n",
    "########################################################################\n",
    "\n",
    "def encode_means_with_cv( X, y, group_col, n_splits ):\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    split_info = [ x for x in kf.split(X) ]\n",
    "\n",
    "    if isinstance( y, np.ndarray ):\n",
    "        # Make sure the target values are a 1-d array\n",
    "        y = y.ravel()\n",
    "        \n",
    "    encoded_feature = pd.Series( np.nan * np.ones_like(y), index = X[group_col] )\n",
    "\n",
    "    for splt in split_info:\n",
    "        # Get the test and train indices for the current fold\n",
    "        idx_train, idx_test = splt\n",
    "\n",
    "        # Get the test and train data\n",
    "        X_train = X.iloc[idx_train,:]\n",
    "        y_train = y[idx_train]\n",
    "        X_test = X.iloc[idx_test,:]\n",
    "        y_test = y[idx_test]\n",
    "\n",
    "        # Put the means into the output vector\n",
    "        data = ( X_train, y_train, X_test, y_test )\n",
    "        encoded_feature.iloc[idx_test] = encode_means_from_test_train_split( data, group_col )\n",
    "\n",
    "    # Fill missing values with the global mean\n",
    "    encoded_feature = encoded_feature.fillna( np.nanmean( y ) )\n",
    "    \n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_means_from_test_train_split( data, group_col):\n",
    "    \n",
    "    # expand the input train/test data (we don't ever use the TARGET value from the test set 'y_test')\n",
    "    X_train, y_train, X_test, _ = data\n",
    "    \n",
    "    # Combine the TARGET column and features together\n",
    "    train_data = pd.concat( [X_train, pd.DataFrame( y_train, index=X_train.index.values, columns=['TARGET'] ) ], axis=1 )\n",
    "    test_data = X_test\n",
    "        \n",
    "    # Get item IDs common to both test and train, and also those just found in the test set\n",
    "    common_ids = set(test_data[group_col]).intersection( set(train_data[group_col]) )\n",
    "    missing_ids = set(test_data[group_col]).difference(common_ids)\n",
    "\n",
    "    # Construct a dictionary mapping item IDs from the test set to their means in the train set\n",
    "    train_means = train_data.groupby(group_col)['TARGET'].mean()    \n",
    "    common_means = pd.Series( [ train_means[x] for x in list(common_ids) ], index=pd.Index(common_ids, dtype='int64' ) )\n",
    "    missing_means = pd.Series( np.nan * np.ones_like(missing_ids), index=pd.Index(list(missing_ids), dtype='int32' ) )\n",
    "    all_means = dict(pd.concat( [ common_means, missing_means ] ) )\n",
    "\n",
    "    encoded_features_test = np.array( [ all_means[x] for x in test_data[group_col] ], dtype='float32' )\n",
    "    \n",
    "    # Fill missing values with the global mean\n",
    "    encoded_features_test[ np.isnan(encoded_features_test) ] = np.nanmean( y_train )\n",
    "    \n",
    "    return encoded_features_test    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding mean on dataset 0 for column TARGET_mean_item_id...\n",
      "Encoding mean on dataset 0 for column TARGET_mean_shop_id...\n",
      "Encoding mean on dataset 0 for column TARGET_mean_item_category_id...\n",
      "Encoding mean on dataset 0 for column TARGET_mean_shop_item_id...\n",
      "Encoding mean on dataset 0 for column TARGET_mean_date_block_num...\n",
      "Encoding mean on dataset 0 for column TARGET_mean_month...\n",
      "Encoding mean on dataset 0 for column TARGET_mean_shop_item_never_active...\n",
      "Encoding mean on dataset 0 for column TARGET_mean_shop_item_inactive...\n",
      "Encoding mean on dataset 1 for column TARGET_mean_item_id...\n",
      "Encoding mean on dataset 1 for column TARGET_mean_shop_id...\n",
      "Encoding mean on dataset 1 for column TARGET_mean_item_category_id...\n",
      "Encoding mean on dataset 1 for column TARGET_mean_shop_item_id...\n",
      "Encoding mean on dataset 1 for column TARGET_mean_date_block_num...\n",
      "Encoding mean on dataset 1 for column TARGET_mean_month...\n",
      "Encoding mean on dataset 1 for column TARGET_mean_shop_item_never_active...\n",
      "Encoding mean on dataset 1 for column TARGET_mean_shop_item_inactive...\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "group_cols = [ 'item_id', 'shop_id', 'item_category_id', 'shop_item_id', 'date_block_num', 'month', \n",
    "             'shop_item_never_active', 'shop_item_inactive']\n",
    "\n",
    "for j in range(len(data_sets)):\n",
    "    \n",
    "    ds_j = data_sets[j]\n",
    "    X_train, y_train, X_test, y_test = ds_j\n",
    "    \n",
    "    for group_col in group_cols:\n",
    "\n",
    "        new_col_name = 'TARGET' + '_mean_' + group_col\n",
    "        print('Encoding mean on dataset {} for column {}...'.format(j, new_col_name ) )\n",
    "\n",
    "        if new_col_name not in X_train:\n",
    "            X_train[new_col_name] = encode_means_with_cv( X_train, y_train, group_col, n_splits ).to_numpy()\n",
    "\n",
    "        if new_col_name not in X_test:\n",
    "            X_test[new_col_name] = encode_means_from_test_train_split( ds_j, group_col=group_col)\n",
    "\n",
    "    # Update the datasets with the mean encoded column\n",
    "    data_sets[j] = ( X_train, y_train, X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Construct Forecast\n",
    "\n",
    "We set up a pipeline for managing the forecasts, and make it easy to use a variety of different models.\n",
    "\n",
    "We show the results from forecasting from several simple examples. \n",
    "\n",
    "Our final forecast is done using RandomForestRegressor from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_as_df( yhat, X_test ):\n",
    "\n",
    "    if isinstance( yhat, pd.Series ):\n",
    "        yhat = yhat.to_numpy()\n",
    "\n",
    "    # Add the shop and item ids to the forecast so we can merge it with the test csv later\n",
    "    yhat_mtx = np.hstack( [ yhat.reshape(len(yhat),1), X_test[['shop_id', 'item_id']].to_numpy() ] )\n",
    "    df_fcst = pd.DataFrame( yhat_mtx, columns=['item_cnt_month', 'shop_id', 'item_id'] )\n",
    "    \n",
    "    return df_fcst\n",
    "\n",
    "def write_forecast_to_file( df_fcst, output_file ):    \n",
    "    \n",
    "    # Merge the forecast with the test csv\n",
    "    df_out = df_fcst.merge( csv_data['test'], on=['shop_id', 'item_id'], suffixes=('_fcst', '_test'), how='left' )\n",
    "    \n",
    "    # Only include the ID and item_cnt_month columns for the submission csv\n",
    "    folder = os.path.split( output_file )[0]\n",
    "    \n",
    "    if 'test' == os.path.split(folder)[1]:\n",
    "        df_out = df_out[['ID', 'item_cnt_month']]\n",
    "    else:\n",
    "        df_out = df_out[['shop_id', 'item_id', 'item_cnt_month']]\n",
    "    \n",
    "    # Write the combined csv to file\n",
    "    write_to_file( output_file, df_out )\n",
    "    \n",
    "        \n",
    "def write_to_file( output_file, df ):\n",
    "    \n",
    "    folder_path = os.path.split( output_file )[0]\n",
    "    subfolder_path = os.path.split( folder_path )[0]\n",
    "    \n",
    "    # Create folder to store results, if it is missing\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "\n",
    "    # Create folder for the validation set if it is missing\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "            \n",
    "    # Write the dataframe to csv\n",
    "    df.to_csv( output_file, index=False )\n",
    "    \n",
    "\n",
    "def get_results_directory():\n",
    "\n",
    "    if USE_LOW_MEMORY_VERSION:\n",
    "        results_dir = 'results_small'\n",
    "    else:\n",
    "        results_dir = 'results_big'\n",
    "        \n",
    "    return results_dir\n",
    "\n",
    "def get_results_subdirectory( data_sets, idx=None, level=0 ):\n",
    "    \n",
    "    if level == 0:\n",
    "        if len(data_sets) == 1 + idx:\n",
    "            output_folder = 'test'\n",
    "        else:\n",
    "            output_folder = 'validation_{}'.format(idx)\n",
    "    else:\n",
    "        output_folder = 'ensemble_{}'.format(level)\n",
    "\n",
    "    results_dir = get_results_directory()\n",
    "    folder_path = os.path.join( PROJECT_PATH, results_dir, output_folder )\n",
    "    return folder_path\n",
    "\n",
    "\n",
    "def get_results_file_name(model_name, data_sets, idx=None, level=0 ):\n",
    "   \n",
    "    folder_path = get_results_subdirectory( data_sets, idx=idx, level=level )\n",
    "    output_file = os.path.join( folder_path, model_name + '.csv' )\n",
    "            \n",
    "    return output_file\n",
    "\n",
    "\n",
    "def get_results_for_model( model_name, data_sets ):\n",
    "\n",
    "    rmse = []\n",
    "    for j in range(len(data_sets)):\n",
    "\n",
    "        file_name = get_results_file_name(model_name, data_sets, idx=j )\n",
    "\n",
    "        y_hat = pd.read_csv( file_name, index=False)\n",
    "        y_test = data_sets[j][-1]\n",
    "        \n",
    "        # Calculate the RMSE\n",
    "        if idx == len(data_sets) - 1:\n",
    "            rmse.append( np.nan )\n",
    "        else:\n",
    "            rmse.append( calc_rmse( y_hat, y_test ) )\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def calc_rmse( y_hat, y_test ):\n",
    "    \n",
    "    rmse = np.sqrt( mean_squared_error( y_hat, y_test ) )\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a set of classes for implementing a range of prediction models.\n",
    "\n",
    "The classes are structured in the way used below to accommodate a common set of pre- and post-processing routines, and to provide the same functional access to the fit/predict methods\n",
    "\n",
    "Much of the below is not necessary for the work being submitted here, but is used for some ensemble forecasts that I have worked on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model( base_model_constructor, model_name, data_sets ):\n",
    "\n",
    "    fitted_models = []\n",
    "    for idx in range(len(data_sets)):\n",
    "\n",
    "        model = base_model_constructor()\n",
    "        X_train_full, y_train, X_test_full, y_test = data_sets[idx]\n",
    "\n",
    "        X_train = X_train_full\n",
    "        X_test = X_test_full\n",
    "        \n",
    "        # Fit the model on the train data and make a prediction on the test set\n",
    "        model.fit( X_train, y_train )\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        # Calculate the RMSE\n",
    "        if idx == len(data_sets) - 1:\n",
    "            rmse = np.nan\n",
    "        else:\n",
    "            rmse = calc_rmse( y_hat, y_test )\n",
    "            \n",
    "        print( 'Writing results for data set {}. RMSE is {}'.format(idx, rmse ) )\n",
    "\n",
    "        # Write the output to file\n",
    "        output_file = get_results_file_name(model_name, data_sets, idx=idx )\n",
    "        \n",
    "        df_fcst = get_forecast_as_df( y_hat, X_test )\n",
    "        write_forecast_to_file( df_fcst, output_file )\n",
    "\n",
    "        fitted_models.append(model)\n",
    "        \n",
    "    return fitted_models\n",
    "\n",
    "class Forecast():\n",
    "    \n",
    "    def __init__(self, features=None, preprocess_type='none', params={} ):\n",
    "\n",
    "        if isinstance( features, str ):\n",
    "            self.features = [ features ]\n",
    "        else:\n",
    "            self.features = features\n",
    "        \n",
    "        self.preprocess_type = preprocess_type\n",
    "        \n",
    "        # Set the parameters\n",
    "        if 'params' in params:\n",
    "            self.params = params['params']\n",
    "        else:\n",
    "            self.params = params\n",
    "     \n",
    "    \n",
    "    def fit( self, X, y ):\n",
    "        \n",
    "        # Pre-process the features\n",
    "        X_pp = self._preprocess(X, action='fit' )\n",
    "        \n",
    "        # Fit the model using the pre-processed features\n",
    "        self._model_fit(X_pp, y)\n",
    "        \n",
    "    \n",
    "    def predict( self, X ):\n",
    "        \n",
    "        # Pre-process the features\n",
    "        X_pp = self._preprocess( X, action='predict' )\n",
    "        \n",
    "        # Make the prediction using the model and the pre-processed features\n",
    "        y_hat = self._model_predict(X_pp)        \n",
    "        \n",
    "        # Perform any post-processing\n",
    "        y_hat = self._postprocess( y_hat )\n",
    "        \n",
    "        # Clip the forecast to be in [0,20]\n",
    "        y_hat = np.clip( y_hat, 0, 20 )\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "    def score( self, X, y ):\n",
    "        \n",
    "        y_hat = self.predict(X)\n",
    "        return np.sqrt( mean_squared_error( y_hat, y ) )\n",
    "\n",
    "    \n",
    "    def _model_fit(self, X_pp, y):\n",
    "        \"\"\"Fit the model on the pre-processed data\"\"\"\n",
    "        \n",
    "        self.model.fit(X_pp, y.ravel() )\n",
    "        \n",
    "        \n",
    "    def _model_predict(self, X_pp):\n",
    "        \"\"\"Make the prediction using the model and the pre-processed features\"\"\"\n",
    "\n",
    "        y_hat = self.model.predict( X_pp )\n",
    "        return y_hat\n",
    "        \n",
    "    def _preprocess( self, X, action ):\n",
    "\n",
    "        if self.preprocess_type == 'none':\n",
    "            X_pp = self._preprocess_none(X, action)\n",
    "        elif self.preprocess_type == 'nmf':\n",
    "            X_pp = self._preprocess_nmf(X, action)\n",
    "        elif self.preprocess_type == 'pca':\n",
    "            X_pp = self._preprocess_pca(X, action)\n",
    "        else:\n",
    "            raise ValueError( \"Unknown preprocess_type: {}\".format(self.preprocess_type ) )\n",
    "            \n",
    "        return X_pp\n",
    "            \n",
    "        \n",
    "    def _preprocess_none(self, X, action):\n",
    "        \n",
    "        if self.features is not None:\n",
    "            return X[self.features]\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def _preprocess_nmf( self, X, action ):\n",
    "        \n",
    "        # Floor the values at 0, as NMF assumes all values are non-negative\n",
    "        X = np.maximum( 0, X )\n",
    "        X /= ( X.std(axis=0) + 1e-6)\n",
    "        \n",
    "        if action == 'fit':\n",
    "            N = int(np.round(np.sqrt(X.shape[1])))\n",
    "            self.nm = NMF( n_components=N) \n",
    "            self.nm.fit(X)\n",
    "\n",
    "        X_pp = self.nm.transform(X)\n",
    "        \n",
    "        return X_pp    \n",
    "\n",
    "    def _preprocess_pca( self, X, action ):\n",
    "        \n",
    "        # Normalize the input\n",
    "        X = (X - X.mean(axis=0) ) / ( X.std(axis=0) + 1e-6)\n",
    "\n",
    "        if action == 'fit':\n",
    "            N = int(np.round(np.sqrt(X.shape[1])))                   \n",
    "            self.pc = PCA( n_components=N)\n",
    "            self.pc.fit(X)            \n",
    "\n",
    "        X_pp = self.pc.transform(X)\n",
    "        \n",
    "        return X_pp \n",
    "    \n",
    "    def _postprocess( self, yhat ):\n",
    "        \"\"\"Do any necessary post-processing before returning the final forecast.\"\"\"\n",
    "        \n",
    "        yhat_pp = yhat\n",
    "        return yhat_pp \n",
    "\n",
    "\n",
    "class _BenchmarkModel():\n",
    "    \n",
    "    def __init__(*args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y ):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X ):\n",
    "        # Assume sales stay the same as the previous period's value\n",
    "        return X.to_numpy()\n",
    "    \n",
    "    \n",
    "class Benchmark(Forecast):\n",
    "\n",
    "    def __init__(self, features=['sales_lag_01'], preprocess_type='none', **kwargs ):\n",
    "        \n",
    "        super(Benchmark, self).__init__( features=features, preprocess_type=preprocess_type, **kwargs )\n",
    "        \n",
    "        self.model = _BenchmarkModel(**self.params)\n",
    "    \n",
    "    \n",
    "class LinReg(Forecast):\n",
    "    \n",
    "    def __init__(self,features=['sales_lag_01'], preprocess_type='none', **kwargs ):\n",
    "\n",
    "        super(LinReg, self).__init__( features=features, preprocess_type=preprocess_type, params=kwargs )\n",
    "        self.model = LinearRegression(**self.params)\n",
    "\n",
    "    \n",
    "class RidgeReg(Forecast):\n",
    "        \n",
    "    def __init__(self, features=None, preprocess_type='none', **kwargs ):\n",
    "\n",
    "        super(RidgeReg, self).__init__(features=features, preprocess_type=preprocess_type, **kwargs )\n",
    "\n",
    "        self.model = Ridge(**self.params)\n",
    "\n",
    "\n",
    "class LGBM(Forecast):\n",
    "    \n",
    "    def __init__(self, features=None, preprocess_type='none', **kwargs ):\n",
    "\n",
    "        super(LGBM, self).__init__( features=features, preprocess_type=preprocess_type, **kwargs )\n",
    "        \n",
    "        # Get the categorical features and number of rounds for boosting from the parameters\n",
    "        self.model_params = self.params.copy()\n",
    "        self.cat_features = self.model_params.pop('cat_features', None)\n",
    "        self.num_boost_round = self.model_params.pop('num_boost_round', None)\n",
    "\n",
    "        \n",
    "    def _model_fit(self, X, y ):\n",
    "        \n",
    "        lgb_train = lgb.Dataset( X, label=y.ravel(), categorical_feature=self.cat_features )        \n",
    "        self.model = lgb.train( self.model_params, lgb_train, num_boost_round=self.num_boost_round )\n",
    "            \n",
    "            \n",
    "    def _model_predict(self, X):\n",
    "        y_hat = self.model.predict( X, num_iteration=self.model.best_iteration)        \n",
    "        return y_hat\n",
    "\n",
    "\n",
    "class CatBoost(Forecast):\n",
    "        \n",
    "    def __init__(self, features=None, preprocess_type='none', **kwargs ):\n",
    "\n",
    "        super(CatBoost, self).__init__( features=features, preprocess_type=preprocess_type, **kwargs )\n",
    "        \n",
    "        self.model_params = self.params.copy()\n",
    "        self.cat_features = self.model_params.pop('cat_features', None)\n",
    "        self.plot = self.model_params.pop('plot', None)\n",
    "        \n",
    "        # Construct the model\n",
    "        self.model = CatBoostRegressor( **self.model_params)\n",
    "\n",
    "        \n",
    "    def _model_fit(self, X_pp, y ):\n",
    "\n",
    "        train_data = Pool( X_pp, y.ravel(), cat_features=self.cat_features )\n",
    "        \n",
    "        self.model.fit(\n",
    "            train_data,\n",
    "            logging_level='Silent',\n",
    "            plot=self.plot\n",
    "        )\n",
    "\n",
    "    def _model_predict(self, X_pp ):\n",
    "        \n",
    "        test_data = Pool( X_pp, cat_features=self.cat_features )\n",
    "        \n",
    "        # Check the out-of-sample fit for the validation set\n",
    "        yhat = self.model.predict(test_data)\n",
    "        \n",
    "        return yhat\n",
    "\n",
    "    \n",
    "class RF(Forecast):\n",
    "    \n",
    "    def __init__(self, features=None, preprocess_type='none', **kwargs ):\n",
    "\n",
    "        super(RF, self).__init__( features=features, preprocess_type=preprocess_type, **kwargs )\n",
    "        \n",
    "        # Construct the model\n",
    "        self.model = RandomForestRegressor( **self.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Peform a forecast with the 'Benchmark' model, which just uses the previous months' sales as its prediction for the next month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 0. RMSE is 1.135817050933838\n",
      "Writing results for data set 1. RMSE is nan\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "features = ['sales_lag_01']\n",
    "preprocess_type = 'none'\n",
    "\n",
    "constr = lambda : Benchmark( features=features, preprocess_type=preprocess_type, params=params )\n",
    "models = fit_model( constr, 'benchmark', data_sets )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Next, use a simple model that performs a linear regression using just the previous month's sales as a dependent variable.\n",
    "\n",
    "The forecast is clipped to be in [0,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 0. RMSE is 1.009634256362915\n",
      "Writing results for data set 1. RMSE is nan\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "features = ['sales_lag_01']\n",
    "preprocess_type = 'none'\n",
    "\n",
    "constr = lambda : LinReg( features=features, preprocess_type=preprocess_type, params=params )\n",
    "models = fit_model( constr, 'linreg', data_sets )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "The last example uses ridge regression. We use sklearn's class to normalize the input variables. The value of alpha was found using hyperopt, to minimize the RMSE on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 0. RMSE is 0.9755492608718589\n",
      "Writing results for data set 1. RMSE is nan\n"
     ]
    }
   ],
   "source": [
    "params = { \n",
    "            'alpha' : 1.24, \n",
    "            'normalize' : True\n",
    "         }\n",
    "\n",
    "features = None\n",
    "preprocess_type = 'none'\n",
    "\n",
    "constr = lambda : RidgeReg( params=params, features=features, preprocess_type=preprocess_type )\n",
    "models = fit_model( constr, 'ridgereg', data_sets )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL PREDICTION\n",
    "\n",
    "The final prediction is done using RandomForestRegressor from sklearn.\n",
    "\n",
    "The parameters used here were found using the hyperopt package (see below for the code).\n",
    "\n",
    "The test csv should be written to PROJECT_PATH/results_small/test/randomforest.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results for data set 0. RMSE is 0.9417410742830974\n",
      "Writing results for data set 1. RMSE is nan\n"
     ]
    }
   ],
   "source": [
    "# Random Forest parameters\n",
    "params = {\n",
    "    'criterion': 'mse', \n",
    "    'max_depth': 10, \n",
    "    'max_features': 0.15,\n",
    "    'min_samples_split': 20, \n",
    "    'n_estimators': 250, \n",
    "    'n_jobs': 16\n",
    "}\n",
    "\n",
    "\n",
    "features = None\n",
    "preprocess_type = 'none'\n",
    "\n",
    "constr = lambda : RF( params=params, features=features, preprocess_type=preprocess_type )\n",
    "models = fit_model( constr, 'randomforest', data_sets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Hyperparameter Optimization\n",
    "\n",
    "The following provides the methodology used for optimizing the hyperparameters of RandomForestRegressor, using the hyperopt package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 s, sys: 0 ns, total: 13 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from hyperopt import hp, tpe, fmin, STATUS_OK, Trials\n",
    "\n",
    "def optimize_hyperparameters( model_class, ds, param_search_space, int_params=[], max_evals=20 ):\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = ds\n",
    "\n",
    "    def hyperopt_train_test(param_search_space):\n",
    "        \n",
    "        model = model_class(params=param_search_space)\n",
    "        \n",
    "        model.fit( X_train, y_train )\n",
    "        \n",
    "        # Make sure forecasts are in [0,20]\n",
    "        y_hat = np.clip( model.predict(X_valid), 0, 20 )\n",
    "        \n",
    "        # Return the RMSE\n",
    "        rmse = np.sqrt( mean_squared_error( y_hat, y_valid ) )\n",
    "        return rmse\n",
    "\n",
    "    def f(params): \n",
    "        for p in int_params:\n",
    "            params[p] = int( params[p] )\n",
    "\n",
    "        rmse = hyperopt_train_test(params)\n",
    "        return {'loss': rmse, 'status': STATUS_OK, 'params' : params}\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(f, param_search_space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "\n",
    "    # Get the optimal parameters\n",
    "    losses = np.array( [ x['loss'] for x in trials.results ] )\n",
    "    opt_params = trials.results[np.argmin(losses)]['params']\n",
    "\n",
    "    return opt_params, trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [02:28<00:00, 16.92s/it, best loss: 0.9445014077153435]\n",
      "{'criterion': 'mse', 'max_depth': 10, 'max_features': 0.14409047802273467, 'min_samples_split': 19, 'n_estimators': 50, 'n_jobs': 16}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "# Hyper parameter search\n",
    "######################################################\n",
    "\n",
    "param_search_space = {\n",
    "    'n_jobs' : hp.choice( 'n_jobs', [ 16 ] ),\n",
    "    'criterion' : hp.choice( 'criterion', [ 'mse' ] ),\n",
    "    'n_estimators' : hp.choice( 'n_estimators', [50] ), \n",
    "    'max_depth' : hp.uniform( 'max_depth', 4, 20 ),\n",
    "    'min_samples_split' : hp.uniform( 'min_samples_split', 4, 30 ), \n",
    "    'max_features' : hp.uniform( 'max_features', 0, 1 )\n",
    "}\n",
    "\n",
    "model_class = RF\n",
    "ds = data_sets[0]\n",
    "int_params = [ 'n_estimators', 'max_depth', 'min_samples_split' ]\n",
    "max_evals = 10\n",
    "\n",
    "opt_params, trials = optimize_hyperparameters( \n",
    "                                              model_class=model_class, \n",
    "                                              ds=ds, \n",
    "                                              param_search_space=param_search_space,\n",
    "                                              int_params=int_params, \n",
    "                                              max_evals=max_evals )\n",
    "\n",
    "print( opt_params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
