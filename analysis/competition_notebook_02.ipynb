{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import datetime\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants that we will use\n",
    "PROJECT_PATH = '/home/ubuntu/projects/kaggle-predict-future-sales'\n",
    "\n",
    "TRAIN = 'train'\n",
    "VALID = 'valid'\n",
    "TEST = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw CSV files\n",
    "\n",
    "csv_data = dict()\n",
    "\n",
    "# Import all csv file data\n",
    "csv_data[ 'sales_daily' ] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/sales_train.csv') )\n",
    "csv_data['item_cat'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/item_categories.csv') )\n",
    "csv_data['item'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/items.csv') )\n",
    "csv_data['sub'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/sample_submission.csv') )\n",
    "csv_data['shops'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/shops.csv') )\n",
    "csv_data['test'] = pd.read_csv( os.path.join( PROJECT_PATH, 'input/test.csv') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some new columns and aggregate from daily to monthly\n",
    "############################################################\n",
    "\n",
    "# Add the daily revenue\n",
    "csv_data['sales_daily'][ \"revenue\" ] = csv_data['sales_daily'].item_price * csv_data['sales_daily'].item_cnt_day\n",
    "\n",
    "# Aggregate the monthly data\n",
    "agg_rules = {'item_price' : \"mean\", \"revenue\" : \"sum\", \"item_cnt_day\" : \"sum\" }\n",
    "groupby_cols = [ 'date_block_num', \"item_id\", \"shop_id\" ]\n",
    "\n",
    "# Add the effective item price\n",
    "csv_data['sales_monthly'] = csv_data['sales_daily'].groupby( groupby_cols ).agg( agg_rules ).reset_index()\n",
    "\n",
    "# Rename the column to reflect monthly data\n",
    "csv_data['sales_monthly'].rename( columns={ 'item_cnt_day' : 'item_cnt_month'}, inplace=True )\n",
    "\n",
    "# Add unit price as a column\n",
    "csv_data['sales_monthly']['item_price_unit'] = np.round( csv_data['sales_monthly']['revenue'] / \\\n",
    "                              ( 1e-6 + csv_data['sales_monthly']['item_cnt_month'] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all combinations of shops/items for each date\n",
    "############################################################\n",
    "\n",
    "df_monthly = csv_data['sales_monthly']\n",
    "dates = df_monthly['date_block_num'].unique()\n",
    "\n",
    "df_id = pd.DataFrame( [], columns=['date_block_num', 'shop_id', 'item_id'])\n",
    "for dt in dates:\n",
    "        \n",
    "    df_t = df_monthly[ df_monthly['date_block_num'] == dt ]\n",
    "    uniq_shops = df_t['shop_id'].unique()    \n",
    "    uniq_items = df_t['item_id'].unique()\n",
    "        \n",
    "    new_rows = pd.DataFrame( itertools.product( [dt], uniq_shops, uniq_items ), columns=df_id.columns )\n",
    "    df_id = pd.concat( [ df_id, new_rows ], sort=False, axis=0 )\n",
    "\n",
    "# Join the test IDs to the data frame\n",
    "df_t = csv_data['test'].copy()\n",
    "df_t['date_block_num'] = 1 + df_id['date_block_num'].max()\n",
    "df_id = pd.concat( [ df_id, df_t ], sort=False, axis=0 ).drop(['ID'], axis=1)\n",
    "\n",
    "# Create a data frame using all the shop/item pairs from each month\n",
    "df_sales = df_id.merge( csv_data['sales_monthly'], on=['date_block_num', 'shop_id', 'item_id' ], how='left' )\n",
    "\n",
    "# Set missing values for revenue and item count to 0\n",
    "df_sales.loc[ np.isnan(df_sales['revenue']), ['revenue'] ] = 0\n",
    "df_sales.loc[ np.isnan(df_sales['item_cnt_month']), ['item_cnt_month'] ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns to the data frame\n",
    "############################################################\n",
    "\n",
    "# Add the category id\n",
    "df_sales = df_sales.merge( csv_data['item'], on='item_id' )\n",
    "df_sales = df_sales.drop('item_name', axis=1 )\n",
    "\n",
    "# Add a column combining shop and item, which together with date_block_num is a unique id\n",
    "df_sales['shop_item_id'] = df_sales['shop_id'] + 100 * df_sales['item_id']\n",
    "\n",
    "# Add the month and year\n",
    "df_sales['month'] = 1 + (df_sales['date_block_num'] % 12)\n",
    "df_sales['year'] = 2013 + (df_sales['date_block_num'] // 12)\n",
    "\n",
    "# Add the dates\n",
    "# dates = pd.Series( [ datetime.date( y, m, 15 ) for y in range(2013,2016) for m in range(1, 13) ], \\\n",
    "#                           index=range(0,36) )\n",
    "# df_sales['date'] = df_sales['date_block_num'].map(dates)\n",
    "\n",
    "# Rename the target column and make it the first column\n",
    "target = df_sales.loc[:,['item_cnt_month']]\n",
    "target.columns = [ 'TARGET']\n",
    "df_sales = pd.concat( [ target, df_sales.drop('item_cnt_month', axis=1) ], axis=1 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Make time series out of the monthly  sales\n",
    "pivot_columns = [ 'shop_id', 'item_id']\n",
    "target_col = 'item_cnt_month'\n",
    "grp_table = csv_data['sales_monthly'].groupby( [ 'date_block_num' ] + pivot_columns ).agg( { target_col : 'sum' } ).reset_index()\n",
    "ts_sales_raw = grp_table.pivot_table( target_col, index=\"date_block_num\", columns=pivot_columns )\n",
    "\n",
    "# Set negative values to 0\n",
    "ts_sales_raw = np.maximum( 0, ts_sales_raw )\n",
    "\n",
    "# Make sure the dates are sorted\n",
    "ts_sales_raw.sort_index(axis=0, ascending=True, inplace=True )\n",
    "\n",
    "# Create a version that has missing values filled with 0\n",
    "ts_sales = ts_sales_raw.fillna(0)\n",
    "\n",
    "# Downcast both versions\n",
    "ts_sales_raw = ts_sales_raw.astype('float32')\n",
    "ts_sales = ts_sales.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize a list to store features. We will concatenate these afterwards with hstack\n",
    "feature_vals = []\n",
    "feature_names = []\n",
    "\n",
    "# Just keep the last M months of observations, and convert to a numpy column vector\n",
    "process_features = lambda x : x.to_numpy().ravel()[:,np.newaxis]\n",
    "\n",
    "# Get lagged monthly sales\n",
    "print( 'Calculating lagged sales...')\n",
    "lags = np.arange(1,13)\n",
    "for L in lags:\n",
    "    ts = ts_sales.shift(periods=L-1)\n",
    "    feature_vals.append( process_features(ts) )\n",
    "    feature_names.append( 'sales_lag_{:02}'.format(L) )\n",
    "\n",
    "# Get lagged monthly mean sales\n",
    "print( 'Calculating rolling mean sales...')\n",
    "means = [3, 6, 9, 12]\n",
    "for M in means:\n",
    "    ts = ts_sales.rolling(window=M).mean()\n",
    "    feature_vals.append( process_features(ts) )    \n",
    "    feature_names.append( 'sales_mean_{:02}'.format(M) )\n",
    "\n",
    "# Get 12-month standard deviation\n",
    "print( 'Calculating std. dev. of sales...')\n",
    "means = [6, 12]\n",
    "for M in means:\n",
    "    ts = ts_sales.rolling(window=M).std()\n",
    "    feature_vals.append( process_features(ts) )\n",
    "    feature_names.append( 'sales_std_{:02}'.format(M) )\n",
    "\n",
    "# Get the 12-month quartiles\n",
    "print( 'Calculating quantiles of sales...')\n",
    "for p in [ 0.25, 0.50, 0.75]:\n",
    "    ts = ts_sales.rolling(window=12).quantile(p)\n",
    "    feature_vals.append( process_features(ts) )    \n",
    "    feature_names.append( 'sales_percentile_{:02}_12'.format(int(p * 100) ) )\n",
    "\n",
    "# Add some categorical features\n",
    "print( 'Collecting categorical features...')\n",
    "for j, col in enumerate(ts_sales.columns.names):\n",
    "    ts = np.array([ x[j] for x in ts_sales.columns ] * ts_sales.shape[0]).reshape( ts_sales.shape[0], ts_sales.shape[1])\n",
    "    feature_vals.append( process_features(pd.DataFrame(ts) ) )  \n",
    "    feature_names.append( col )\n",
    "\n",
    "# Add some more categorical features for the date block, year and month\n",
    "ts_dates = np.tile( np.array(ts_sales.index)[:,np.newaxis], ts_sales.shape[1] )\n",
    "feature_vals.append( process_features(pd.DataFrame(ts_dates) ) )\n",
    "feature_names.append( 'date_block_num' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcast floats and ints to 32-bit to save memory\n",
    "################################################################\n",
    "\n",
    "def downcast_dataframe( df ):\n",
    "    for col in df.columns:\n",
    "        if isinstance( df[col ].iloc[0], np.float64 ) or isinstance( df[col ].iloc[0], float ):\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif isinstance( df[col ].iloc[0], np.int64 ) or isinstance( df[col ].iloc[0], int ):\n",
    "            df[col] = df[col].astype('int32')\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_months_since_first_and_last_observation( input_ts ):\n",
    "    \"\"\"Gets the number of months since the first and last observation in each columns, \n",
    "    at each point in time. This function has no 'look-forward' bias.\"\"\"\n",
    "    \n",
    "    # Create an array with the index (0 to T) of any non-zeros entries\n",
    "    month_of_obs = input_ts.to_numpy() * np.arange(1,input_ts.shape[0]+1)[:,np.newaxis]\n",
    "    month_of_obs = month_of_obs.astype('float32')\n",
    "    month_of_obs[ month_of_obs < 0.01 ] = np.nan\n",
    "\n",
    "    # Make a data frame\n",
    "    df_month_of_obs = pd.DataFrame( month_of_obs, columns=input_ts.columns)\n",
    "\n",
    "    # Loop through the time steps and find the months since first/last action at each t\n",
    "    months_since_first_obs = np.nan * np.ones_like(month_of_obs)\n",
    "    months_since_last_obs = np.nan * np.ones_like(month_of_obs)\n",
    "    for t in range( month_of_obs.shape[0] ):\n",
    "        months_since_first_obs[t,:] = t - df_month_of_obs.iloc[:(t+1),:].idxmin(axis=0)\n",
    "        months_since_last_obs[t,:] = t - df_month_of_obs.iloc[:(t+1),:].idxmax(axis=0)    \n",
    "\n",
    "    # Fill NaN's with 999 for shop/items that have never seen a sale\n",
    "    months_since_first_obs[ np.isnan( months_since_first_obs ) ] = 999\n",
    "    months_since_last_obs[ np.isnan( months_since_last_obs ) ] = 999\n",
    "\n",
    "    # Convert back into pandas Dataframe\n",
    "    months_since_first_obs = pd.DataFrame( months_since_first_obs, columns=ts_sales.columns )\n",
    "    months_since_last_obs = pd.DataFrame( months_since_last_obs, columns=ts_sales.columns )\n",
    "    \n",
    "    return months_since_first_obs, months_since_last_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get the number of months since the first and last sale, at each point in time\n",
    "months_since_first_sale, months_since_last_sale = \\\n",
    "        get_months_since_first_and_last_observation( ts_sales_raw )\n",
    "\n",
    "# Add to features\n",
    "feature_vals.append( process_features( months_since_first_sale.astype('float32') ) )\n",
    "feature_names.append( 'months_since_first_sale' )\n",
    "feature_vals.append( process_features( months_since_last_sale.astype('float32') ) )\n",
    "feature_names.append( 'months_since_last_sale' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Join the features into a pandas data frame\n",
    "features_list = [ x for x in feature_vals ]\n",
    "df_calc = pd.DataFrame( np.hstack(features_list), columns=feature_names)\n",
    "\n",
    "# We must shift the date_block_num forward by 1 for the calculated columns, \n",
    "#    since these are not available until the next month\n",
    "df_calc['date_block_num'] = 1 + df_calc['date_block_num'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Combine the sales data with the calculated features\n",
    "df_full = df_sales.merge( df_calc, on=['date_block_num', 'shop_id', 'item_id' ], how='left' )\n",
    "df_full = downcast_dataframe(df_full)\n",
    "\n",
    "# Remove the first 12 months, since features using 12 months of data will be NaN\n",
    "df_full = df_full[ df_full['date_block_num'] >= 12 ]\n",
    "\n",
    "# Clean up missing values in the calculated columns\n",
    "calc_sales_cols = list( df_calc.columns[ [ x.startswith('sales_') for x in df_calc.columns ] ] )\n",
    "df_full[calc_sasles_cols] = df_full[calc_sales_cols].fillna(0)\n",
    "\n",
    "df_full['months_since_first_sale'] = df_full['months_since_first_sale'].fillna(999).values\n",
    "df_full['months_since_last_sale'] = df_full['months_since_last_sale'].fillna(999).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional grouped features \n",
    "#############################################\n",
    "\n",
    "def create_group_features( df, group_col, agg_rule, target_col ):\n",
    "    new_col_name = target_col + '_' + agg_rule + '_by_' + group_col \n",
    "    vals = df_full.groupby( [ 'date_block_num', group_col ] )[ target_col ].transform(agg_rule)\n",
    "    df[new_col_name] = vals.astype('float32')    \n",
    "    return df\n",
    "\n",
    "# Create a new column representing the total sales over the past 12 months\n",
    "df_full['sales_total_12'] = 12 * df_full['sales_mean_12']\n",
    "\n",
    "for target_col in [ 'sales_lag_01', 'sales_total_12' ]:\n",
    "    for agg_rule in [ 'sum', 'mean', 'count' ]:\n",
    "        for group_col in [ 'item_id', 'shop_id', 'item_category_id' ]:\n",
    "            df_full = create_group_features( df_full, group_col, agg_rule, target_col )\n",
    "\n",
    "for group_col in [ 'item_id', 'shop_id', 'item_category_id']:\n",
    "    df_full = create_group_features( df_full, group_col, 'min', 'months_since_first_sale' )\n",
    "    df_full = create_group_features( df_full, group_col, 'min', 'months_since_last_sale' )   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional categorical features\n",
    "#############################################\n",
    "\n",
    "df_full['shop_item_never_active'] = ( df_full['months_since_first_sale'] == 999 ).astype('int32')\n",
    "df_full['shop_item_inactive'] = ( df_full['months_since_last_sale'] > 12 ).astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base set of features that will be used for training and validation\n",
    "########################################################################\n",
    "\n",
    "# Drop the revenue and price columns\n",
    "df_base = df_full.drop( [ 'item_price', 'item_price_unit', 'revenue' ], axis=1 )\n",
    "\n",
    "# Only keep the past 12 periods for testing\n",
    "df_base = df_base[ df_base['date_block_num' ] >= 22 ]\n",
    "df_base = downcast_dataframe( df_base )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw features to a HDF5 file\n",
    "print( 'File size: {:02} MB'.format( np.round( df_base.memory_usage().sum() / 1e6 ) ) )\n",
    "# file_name = os.path.join( PROJECT_PATH, 'preprocessed_data/base_features.h5')\n",
    "# df_base.to_hdf(file_name, key='base_features')     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw features from a HDF5 file\n",
    "# file_name = os.path.join( PROJECT_PATH, 'preprocessed_data/raw_features.h5')\n",
    "# df_full = pd.read_hdf(file_name, key='raw_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_cols = df_base.columns[ [ x.startswith( 'sales_lag_') or x.startswith( 'sales_mean_') for x in df_base.columns ] ]\n",
    "X = df_base[ sales_cols ]\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pc = PCA( n_components=10 )\n",
    "pc.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for mean encoding\n",
    "########################################################################\n",
    "\n",
    "def encode_means_with_cv( df, target_col, group_col, n_splits ):\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "    split_info = [ x for x in kf.split(df) ]\n",
    "\n",
    "    mu = np.nanmean( df[target_col] )\n",
    "    encoded_feature = pd.Series( np.nan * np.ones_like(df[target_col]), index = df[group_col] )\n",
    "\n",
    "    for splt in split_info:\n",
    "        # Get the test and train indices for the current fold\n",
    "        idx_train, idx_test = splt\n",
    "\n",
    "        # Get the test and train data\n",
    "        train_data = df.iloc[idx_train,:]\n",
    "        test_data = df.iloc[idx_test,:]\n",
    "\n",
    "        # Put the means into the output vector\n",
    "        encoded_feature.iloc[idx_test] = encode_means_from_test_train_split( train_data, test_data, target_col, group_col)\n",
    "\n",
    "    # Fill missing values with the global mean\n",
    "    encoded_feature = encoded_feature.fillna(mu)\n",
    "    \n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_means_from_test_train_split( train_data, test_data, target_col, group_col):\n",
    "    \n",
    "    # Get item IDs common to both test and train, and also those just found in the test set\n",
    "    common_ids = set(test_data[group_col]).intersection( set(train_data[group_col]) )\n",
    "    missing_ids = set(test_data[group_col]).difference(common_ids)\n",
    "\n",
    "    # Construct a dictionary mapping item IDs from the test set to their means in the train set\n",
    "    train_means = train_data.groupby(group_col)[target_col].mean()    \n",
    "    common_means = pd.Series( [ train_means[x] for x in list(common_ids) ], index=pd.Index(common_ids, dtype='int64' ) )\n",
    "    missing_means = pd.Series( np.nan * np.ones_like(missing_ids), index=pd.Index(list(missing_ids), dtype='int32' ) )\n",
    "    all_means = dict(pd.concat( [ common_means, missing_means ] ) )\n",
    "\n",
    "    encoded_features_test = [ all_means[x] for x in test_data[group_col] ]    \n",
    "    return encoded_features_test    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get different datasets, in chronological order\n",
    "# The test set is always the next period after the end of the train set.\n",
    "##########################################################################\n",
    "\n",
    "def get_validation_set( df, idx, min_train_size=10 ):\n",
    "\n",
    "    date_blocks = df['date_block_num']\n",
    "    uniq_date_blocks = np.sort( date_blocks.unique() )\n",
    "\n",
    "    n_datasets = len(uniq_date_blocks) - min_train_size - 1\n",
    "    if idx >= 0:\n",
    "        split_date = uniq_date_blocks[min_train_size + idx]\n",
    "    else:\n",
    "        split_date = uniq_date_blocks[idx]    \n",
    "\n",
    "    xtrain = df[ date_blocks < split_date].drop('TARGET', axis=1 )\n",
    "    ytrain = df[ date_blocks < split_date ]['TARGET'][:,np.newaxis]\n",
    "    xtest = df[ date_blocks == split_date ].drop('TARGET', axis=1 )\n",
    "    ytest = df[ date_blocks == split_date ]['TARGET'][:,np.newaxis]\n",
    "\n",
    "    return xtrain, ytrain, xtest, ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "group_cols = [ 'item_id', 'shop_id', 'item_category_id', 'shop_item_id', 'date_block_num', 'month' ]\n",
    "\n",
    "for group_col in group_cols:\n",
    "\n",
    "    new_col_name = 'TARGET' + '_mean_' + group_col\n",
    "    print('{}'.format(datetime.datetime.now()) + ' : ' + new_col_name )\n",
    "\n",
    "    if new_col_name not in df_dict[TRAIN]:\n",
    "        df_dict[TRAIN][new_col_name] = encode_means_with_cv( df_dict[TRAIN], \\\n",
    "                                    target_col='TARGET', group_col=group_col, n_splits=n_splits ).to_numpy()\n",
    "\n",
    "    if new_col_name not in df_dict[VALID]:\n",
    "        df_dict[VALID][new_col_name] = encode_means_from_test_train_split( df_dict[TRAIN], df_dict[VALID], \\\n",
    "                                    target_col='TARGET', group_col=group_col)    \n",
    "\n",
    "    if new_col_name not in df_dict[TEST]:\n",
    "        test_data = pd.concat( [ df_dict[TRAIN], df_dict[VALID] ])                \n",
    "        df_dict[TEST][new_col_name] = encode_means_from_test_train_split( test_data, df_dict[VALID], \\\n",
    "                                    target_col='TARGET', group_col=group_col)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some train/validation/test data sets\n",
    "##########################################################################\n",
    "\n",
    "data_sets = []\n",
    "for j in range(100):\n",
    "    try:\n",
    "        xtrain, ytrain, xtest, ytest = get_validation_set( df_full, j )\n",
    "        data_sets.append( ( xtrain, ytrain, xtest, ytest ) )\n",
    "    except IndexError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python37)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
